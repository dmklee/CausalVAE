{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.z_dim = z_dim\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc31 = nn.Linear(1000, z_dim)  # mu values\n",
    "        self.fc32 = nn.Linear(1000, z_dim)  # sigma values\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        # define the forward computation on the image xs and label ys\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        xs = xs.reshape(-1, self.image_dim)\n",
    "        #now concatenate the image and label\n",
    "        inputs = torch.cat((xs,ys), -1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc31(hidden2)\n",
    "        z_scale = torch.exp(self.fc32(hidden2))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        hidden_dim = 1000\n",
    "        self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, image_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, zs, ys):\n",
    "        # define the forward computation on the latent z and label y\n",
    "        # first concatenate z and y\n",
    "        inputs = torch.cat((zs, ys),-1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        hidden3 = self.softplus(self.fc3(hidden2))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc4(hidden3))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.image_dim = 64**2\n",
    "        self.label_shape = np.array((1,3,6,40,32,32))\n",
    "        self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))\n",
    "        self.label_dim = np.sum(self.label_shape)\n",
    "        self.z_dim = 50                                    \n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the paramters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # dsprites label (latent)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"cvae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "    \n",
    "            loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label \n",
    "        loc, scale are given by a neural network `encoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample (and score) the latent handwriting-style with the variational\n",
    "            # distribution q(z|x) = normal(loc(x),scale(x))\n",
    "    \n",
    "            loc, scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "            \n",
    "    def remap_y(self, ys):\n",
    "        new_ys = []\n",
    "        options = dict(dtype=ys.dtype, device=ys.device)\n",
    "        for i, label_length in enumerate(self.label_shape):\n",
    "            prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)\n",
    "            new_ys.append(pyro.sample(\"y_%s\" % self.label_names[i], dist.OneHotCategorical(prior), \n",
    "                                   obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))\n",
    "        new_ys = torch.cat(new_ys, -1)\n",
    "        return new_ys.to(torch.float32)\n",
    "            \n",
    "    def reconstruct_image(self, xs, ys):\n",
    "        # backward\n",
    "        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
    "        # forward\n",
    "        loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "        return dist.Bernoulli(loc).to_event(1).sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):\n",
    "    train_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(train_y.astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    test_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(test_y.astype(np.float32))\n",
    "    )    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\"train\":train_loader, \"test\":test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip = np.load(\n",
    "    'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
    "    encoding = 'bytes',\n",
    "    allow_pickle=True\n",
    ")\n",
    "imgs = dataset_zip['imgs']\n",
    "labels = dataset_zip['latents_classes']\n",
    "label_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
    "label_names = dataset_zip['metadata'][()][b'latents_names']\n",
    "\n",
    "# Sample imgs randomly\n",
    "indices_sampled = np.arange(imgs.shape[0])\n",
    "np.random.shuffle(indices_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "labels_sampled = labels[indices_sampled]\n",
    "\n",
    "data_loaders = setup_data_loaders(\n",
    "    imgs_sampled[1000:],\n",
    "    imgs_sampled[:1000],\n",
    "    labels_sampled[1000:],\n",
    "    labels_sampled[:1000],\n",
    "    batch_size=256,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ae45f1988e46e19ccc4b27f64dc946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='shape', max=2), IntSlider(value=4, description='scale', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.find_in_dataset(shape, scale, orient, posX, posY)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_names = ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "y_shapes = np.array((3,6,40,32,32))\n",
    "img_dict = {}\n",
    "for i, img in enumerate(imgs_sampled):\n",
    "    img_dict[tuple(labels_sampled[i])] = img\n",
    "    \n",
    "def find_in_dataset(shape, scale, orient, posX, posY):\n",
    "    fig = plt.figure()\n",
    "    img = img_dict[(0, shape, scale, orient, posX, posY)]\n",
    "    plt.imshow(img.reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(find_in_dataset, shape=widgets.IntSlider(min=0, max=2, step=1, value=npr.randint(2)),\n",
    "                          scale=widgets.IntSlider(min=0, max=5, step=1, value=npr.randint(5)),\n",
    "                            orient=widgets.IntSlider(min=0, max=39, step=1, value=npr.randint(39)),\n",
    "                            posX=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)),\n",
    "                            posY=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_specific_data(args=dict(), cuda=False):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        cuda - bool to indicate whether the output should be placed on GPU\n",
    "    '''\n",
    "    names_dict = {'shape': 1, 'scale': 2, 'orientation': 3, 'posX': 4, 'posY': 5}\n",
    "    selected_ind = np.ones(imgs.shape[0], dtype=bool)\n",
    "    for k,v in args.items():\n",
    "        col_id = names_dict[k]\n",
    "        selected_ind = np.bitwise_and(selected_ind, labels[:, col_id] == v)\n",
    "    ind = np.random.choice(np.arange(imgs.shape[0])[selected_ind])\n",
    "    x = torch.from_numpy(imgs[ind].reshape(1,64**2).astype(np.float32))\n",
    "    y = torch.from_numpy(labels[ind].reshape(1,6).astype(np.float32))\n",
    "    if not cuda:\n",
    "        return x,y\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    return x,y\n",
    "\n",
    "def plot_image(x):\n",
    "    x = x.cpu()\n",
    "    plt.figure()\n",
    "    plt.imshow(x.reshape(64,64), interpolation='nearest', cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "def see_specific_image(args=dict(), verbose=True):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        verbose - bool to indicate whether the full class label should be written \n",
    "                    as the title of the plot\n",
    "    '''\n",
    "    x,y = get_specific_data(args, cuda=False)\n",
    "    plot_image(x)\n",
    "    if verbose:\n",
    "        string = ''\n",
    "        for i, s in enumerate(['Shape', 'Scale', 'Orientation', 'PosX', 'PosY']):\n",
    "            string += '%s: %d, ' % (s, int(y[0][i+1]))\n",
    "            if i == 2:\n",
    "                string = string[:-2] + '\\n'\n",
    "        plt.title(string[:-2])\n",
    "        \n",
    "def compare_reconstruction(original, recon):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array for recon image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(recon.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstruction')\n",
    "    \n",
    "def compare_to_density(original, recons):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array of multiple recon images\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(torch.mean(recons.cpu(), 0).reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstructions')\n",
    "\n",
    "        \n",
    "see_specific_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training or Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for xs,ys in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(xs, ys)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for xs, ys in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(xs, ys)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 0\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### FOR SAVING AND LOADING MODEL\n",
    "################################\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "PATH = \"trained_model.save\"\n",
    "\n",
    "# new model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# save current model\n",
    "# torch.save(vae.state_dict(), PATH)\n",
    "\n",
    "# to load params from trained model\n",
    "vae = CVAE(use_cuda=USE_CUDA)\n",
    "vae.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935fefbb79a14937bf871bd1a5a0bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "\n",
    "VERBOSE = True\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in pbar:\n",
    "    total_epoch_loss_train = train(svi, data_loaders[\"train\"], use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    if VERBOSE:\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, data_loaders[\"test\"], use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        if VERBOSE:\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reconstruction accuracy of trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c71e47ca164895b547e33f005f85e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='data id', max=256), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(data_loaders[\"train\"])\n",
    "xs, ys = next(data_iter)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cuda()\n",
    "    ys = ys.cuda()\n",
    "rs = vae.reconstruct_image(xs, ys)\n",
    "\n",
    "def f(i):\n",
    "    compare_reconstruction(xs[i], rs[i])\n",
    "    \n",
    "interact(f, i=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0,description='data id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Structural Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4,  10,  50,  82, 114])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dims = vae.label_shape\n",
    "label_dim_offsets = np.cumsum(label_dims)\n",
    "label_dim_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCM():\n",
    "    def __init__(self, vae, mu, sigma):\n",
    "        self.vae = vae\n",
    "        \n",
    "        mu = mu.cpu()\n",
    "        sigma = sigma.cpu()\n",
    "        \n",
    "        # these are used for f_X\n",
    "        label_dims = vae.label_shape\n",
    "        label_dim_offsets = np.cumsum(label_dims)\n",
    "        \n",
    "        def f_X(Y, Z, N):\n",
    "            zs = Z.cuda()\n",
    "            \n",
    "            # convert the labels to one hot\n",
    "            ys = [torch.tensor([0])]\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[0]), int(label_dims[1])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[1]), int(label_dims[2])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[2]), int(label_dims[3])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[3]), int(label_dims[4])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[4]), int(label_dims[5])))\n",
    "            ys = torch.cat(ys).to(torch.float32).reshape(1,-1).cuda()\n",
    "            \n",
    "            p = vae.decoder.forward(zs, ys)\n",
    "            return (N < p.cpu()).type(torch.float)\n",
    "        \n",
    "        def f_Y(N):\n",
    "            return torch.argmax(N).item()\n",
    "        \n",
    "        def f_Z(N):\n",
    "            return N * sigma + mu\n",
    "        \n",
    "        def model(noise):\n",
    "            N_X = pyro.sample( 'N_X', noise['N_X'] )\n",
    "            # There are 5 Y variables and they will be\n",
    "            # denoted using the index in the sequence \n",
    "            # that they are stored in as vae.label_names:\n",
    "            # ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "            N_Y_1 = pyro.sample( 'N_Y_1', noise['N_Y_1'] )\n",
    "            N_Y_2 = pyro.sample( 'N_Y_2', noise['N_Y_2'] )\n",
    "            N_Y_3 = pyro.sample( 'N_Y_3', noise['N_Y_3'] )\n",
    "            N_Y_4 = pyro.sample( 'N_Y_4', noise['N_Y_4'] )\n",
    "            N_Y_5 = pyro.sample( 'N_Y_5', noise['N_Y_5'] ) \n",
    "            N_Z = pyro.sample( 'N_Z', noise['N_Z'] )\n",
    "             \n",
    "            Z = pyro.sample('Z', dist.Normal( f_Z( N_Z ), 1e-1) )\n",
    "            Y_1_mu = f_Y(N_Y_1)\n",
    "            Y_2_mu = f_Y(N_Y_2)\n",
    "            Y_3_mu = f_Y(N_Y_3)\n",
    "            Y_4_mu = f_Y(N_Y_4)\n",
    "            Y_5_mu = f_Y(N_Y_5)\n",
    "            Y_1 = pyro.sample('Y_1', dist.Normal( Y_1_mu, 1e-1) )\n",
    "            Y_2 = pyro.sample('Y_2', dist.Normal( Y_2_mu, 1e-1) )\n",
    "            Y_3 = pyro.sample('Y_3', dist.Normal( Y_3_mu, 1e-1) )\n",
    "            Y_4 = pyro.sample('Y_4', dist.Normal( Y_4_mu, 1e-1) )\n",
    "            Y_5 = pyro.sample('Y_5', dist.Normal( Y_5_mu, 1e-1) )\n",
    "            Y_mu = (Y_1_mu, Y_2_mu, Y_3_mu, Y_4_mu, Y_5_mu)\n",
    "            X = pyro.sample('X', dist.Normal( f_X( Y_mu, Z, N_X ), 1e-1) )\n",
    "            \n",
    "            noise_samples = N_X, (N_Y_1, N_Y_2, N_Y_3, N_Y_4, N_Y_5), N_Z\n",
    "            variable_samples = X, (Y_1, Y_2, Y_3, Y_4, Y_5), Z\n",
    "            \n",
    "            return variable_samples, noise_samples\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.noise = {\n",
    "            'N_X'   : dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)),\n",
    "            'N_Z'   : dist.Normal(torch.zeros(vae.z_dim), torch.ones(vae.z_dim)),\n",
    "            'N_Y_1' : dist.Uniform(torch.zeros(label_dims[1]),torch.ones(label_dims[1])),\n",
    "            'N_Y_2' : dist.Uniform(torch.zeros(label_dims[2]),torch.ones(label_dims[2])),\n",
    "            'N_Y_3' : dist.Uniform(torch.zeros(label_dims[3]),torch.ones(label_dims[3])),\n",
    "            'N_Y_4' : dist.Uniform(torch.zeros(label_dims[4]),torch.ones(label_dims[4])),\n",
    "            'N_Y_5' : dist.Uniform(torch.zeros(label_dims[5]),torch.ones(label_dims[5]))            \n",
    "        }\n",
    "        \n",
    "    def update_noise_svi(self, obs_data, initial_noise):\n",
    "        def guide(noise):\n",
    "            noise_terms = list(noise.keys())\n",
    "            \n",
    "            \n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.model(self.noise)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check that do-operation should work on trained network\n",
    "\n",
    "At one point I was concerned that the network was encoding too much information in the latent encoding z and thus conditioning on the labels wasnt enough to change the output.  According to the results below, that is not the case.  We should be able to condition on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  2.,  1., 16., 28., 15.]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "original, y_original = get_specific_data(cuda=True)\n",
    "print(y_original)\n",
    "mu, sigma = vae.encoder.forward(x,vae.remap_y(y_original))\n",
    "B = 50\n",
    "zs = torch.cat([dist.Normal(mu.cpu(), sigma.cpu()).sample() for a in range(B)], 0)\n",
    "ys = torch.cat([vae.remap_y(y_original) for a in range(B)], 0)\n",
    "rs = vae.decoder.forward(zs.cuda(), ys).detach()\n",
    "compare_to_density(original,rs)\n",
    "\n",
    "y_new = torch.tensor(y_original)\n",
    "y_new[0,1] = (y[0,1] + 1) % 2\n",
    "zs = torch.cat([dist.Normal(mu.cpu(), sigma.cpu()).sample() for a in range(B)], 0)\n",
    "ys = torch.cat([vae.remap_y(y_new) for a in range(B)], 0)\n",
    "rs = vae.decoder.forward(zs.cuda(), ys).detach()\n",
    "compare_to_density(original,rs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
