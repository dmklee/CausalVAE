{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions.constraints as constraints\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam, SGD\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.z_dim = z_dim\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc31 = nn.Linear(1000, z_dim)  # mu values\n",
    "        self.fc32 = nn.Linear(1000, z_dim)  # sigma values\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        # define the forward computation on the image xs and label ys\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        xs = xs.reshape(-1, self.image_dim)\n",
    "        #now concatenate the image and label\n",
    "        inputs = torch.cat((xs,ys), -1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc31(hidden2)\n",
    "        z_scale = torch.exp(self.fc32(hidden2))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        hidden_dim = 1000\n",
    "        self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, image_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, zs, ys):\n",
    "        # define the forward computation on the latent z and label y\n",
    "        # first concatenate z and y\n",
    "        inputs = torch.cat((zs, ys),-1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        hidden3 = self.softplus(self.fc3(hidden2))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc4(hidden3))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.image_dim = 64**2\n",
    "        self.label_shape = np.array((3,6,40,32,32))\n",
    "        self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))\n",
    "#         self.label_dim = np.sum(self.label_shape)\n",
    "        self.label_dim = self.label_shape.size\n",
    "        self.z_dim = 50                                    \n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the paramters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # dsprites label (latent)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"cvae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "    \n",
    "            loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label \n",
    "        loc, scale are given by a neural network `encoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample (and score) the latent handwriting-style with the variational\n",
    "            # distribution q(z|x) = normal(loc(x),scale(x))\n",
    "    \n",
    "            loc, scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "            \n",
    "    def remap_y(self, ys):\n",
    "        return ys\n",
    "#         new_ys = []\n",
    "#         options = dict(dtype=ys.dtype, device=ys.device)\n",
    "#         for i, label_length in enumerate(self.label_shape):\n",
    "#             prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)\n",
    "#             new_ys.append(pyro.sample(\"y_%s\" % self.label_names[i], dist.OneHotCategorical(prior), \n",
    "#                                    obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))\n",
    "#         new_ys = torch.cat(new_ys, -1)\n",
    "#         return new_ys.to(torch.float32)\n",
    "            \n",
    "    def reconstruct_image(self, xs, ys):\n",
    "        # backward\n",
    "        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
    "        # forward\n",
    "        loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "        return dist.Bernoulli(loc).to_event(1).sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):\n",
    "    train_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(train_y.astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    test_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(test_y.astype(np.float32))\n",
    "    )    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\"train\":train_loader, \"test\":test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip = np.load(\n",
    "    'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
    "    encoding = 'bytes',\n",
    "    allow_pickle=True\n",
    ")\n",
    "imgs = dataset_zip['imgs']\n",
    "labels = dataset_zip['latents_classes']\n",
    "values = dataset_zip['latents_values']\n",
    "values = values[:,1:]\n",
    "values[:, 0] -= 1\n",
    "label_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
    "label_names = dataset_zip['metadata'][()][b'latents_names']\n",
    "\n",
    "# Sample imgs randomly\n",
    "indices_sampled = np.arange(imgs.shape[0])\n",
    "np.random.shuffle(indices_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "labels_sampled = labels[indices_sampled]\n",
    "values_sampled = values[indices_sampled]\n",
    "\n",
    "data_loaders = setup_data_loaders(\n",
    "    imgs_sampled[1000:],\n",
    "    imgs_sampled[:1000],\n",
    "    values_sampled[1000:],\n",
    "    values_sampled[:1000],\n",
    "    batch_size=256,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64cf33ff90f4558a5a7bb29ad117456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='shape', max=2), IntSlider(value=3, description='scale', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.find_in_dataset(shape, scale, orient, posX, posY)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_names = ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "# y_shapes = np.array((3,6,40,32,32))\n",
    "img_dict = {}\n",
    "for i, img in enumerate(imgs_sampled):\n",
    "    img_dict[tuple(labels_sampled[i])] = img\n",
    "    \n",
    "def find_in_dataset(shape, scale, orient, posX, posY):\n",
    "    fig = plt.figure()\n",
    "    img = img_dict[(0, shape, scale, orient, posX, posY)]\n",
    "    plt.imshow(img.reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(find_in_dataset, shape=widgets.IntSlider(min=0, max=2, step=1, value=npr.randint(2)),\n",
    "                          scale=widgets.IntSlider(min=0, max=5, step=1, value=npr.randint(5)),\n",
    "                            orient=widgets.IntSlider(min=0, max=39, step=1, value=npr.randint(39)),\n",
    "                            posX=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)),\n",
    "                            posY=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9074bc33c103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0msee_specific_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-9074bc33c103>\u001b[0m in \u001b[0;36msee_specific_image\u001b[0;34m(args, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Scale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Orientation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PosX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PosY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mstring\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'%s: %d, '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 0 with size 5"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAENCAYAAAAVEjAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA6VJREFUeJzt20FOw0AQAEEW5f9fHq4QAUpHiteGqlty2lNrZm2vmXkDeNT77gMA1yIaQCIaQCIaQCIaQCIaQCIaQCIaQCIaQCIaQCIaQCIaQHLbfYBnrLV8ZQcvNjPru/9NGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkAiGkBy230A/p+Z+fJ7rbXpJDzDpAEkogEk1hO2u19XfmKNOQeTBpCIBpCIBpC40+Ay3H2cg0kDSEQDSKwn/DneOH0tkwaQiAaQiAaQuNPgEI8+LuX8TBpAIhpAIhpAIhpAIhpAIhpAIhpAIhpAIhpA4o1QLsPXqudg0gAS0QAS6wmH+Lxa+Hjt2kwaQCIaQCIaQOJOg8PdPzr97Y7DY9bzMWkAiWgAifWE7awg12LSABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRABLRAJI1M7vPAFyISQNIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIRANIPgBsUCEtykq5bwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_specific_data(args=dict(), cuda=False):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        cuda - bool to indicate whether the output should be placed on GPU\n",
    "    '''\n",
    "    names_dict = {'shape': 1, 'scale': 2, 'orientation': 3, 'posX': 4, 'posY': 5}\n",
    "    selected_ind = np.ones(imgs.shape[0], dtype=bool)\n",
    "    for k,v in args.items():\n",
    "        col_id = names_dict[k]\n",
    "        selected_ind = np.bitwise_and(selected_ind, labels[:, col_id] == v)\n",
    "    ind = np.random.choice(np.arange(imgs.shape[0])[selected_ind])\n",
    "    x = torch.from_numpy(imgs[ind].reshape(1,-1).astype(np.float32))\n",
    "    y = torch.from_numpy(values[ind].reshape(1,-1).astype(np.float32))\n",
    "    if not cuda:\n",
    "        return x,y\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    return x,y\n",
    "\n",
    "def plot_image(x):\n",
    "    x = x.cpu()\n",
    "    plt.figure()\n",
    "    plt.imshow(x.reshape(64,64), interpolation='nearest', cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "def see_specific_image(args=dict(), verbose=True):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        verbose - bool to indicate whether the full class label should be written \n",
    "                    as the title of the plot\n",
    "    '''\n",
    "    x,y = get_specific_data(args, cuda=False)\n",
    "    plot_image(x)\n",
    "    if verbose:\n",
    "        string = ''\n",
    "        for i, s in enumerate(['Shape', 'Scale', 'Orientation', 'PosX', 'PosY']):\n",
    "            string += '%s: %d, ' % (s, int(y[0][i+1]))\n",
    "            if i == 2:\n",
    "                string = string[:-2] + '\\n'\n",
    "        plt.title(string[:-2])\n",
    "        \n",
    "def compare_reconstruction(original, recon):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array for recon image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(recon.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstruction')\n",
    "    \n",
    "def compare_to_density(original, recons):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array of multiple recon images\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(torch.mean(recons.cpu(), 0).reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstructions')\n",
    "\n",
    "        \n",
    "see_specific_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training or Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for xs,ys in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(xs, ys)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for xs, ys in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(xs, ys)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### FOR SAVING AND LOADING MODEL\n",
    "################################\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "PATH = \"trained_model_float_labels.save\"\n",
    "\n",
    "# new model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# save current model\n",
    "torch.save(vae.state_dict(), PATH)\n",
    "\n",
    "# to load params from trained model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "# vae.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 5\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896abf78f69f4f138235f22d3d797111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 32.9570\n",
      "[epoch 000] average test loss: 34.0112\n",
      "[epoch 001]  average training loss: 29.8739\n",
      "[epoch 002]  average training loss: 28.3895\n",
      "[epoch 003]  average training loss: 27.5619\n",
      "[epoch 004]  average training loss: 27.0723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "\n",
    "VERBOSE = True\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in pbar:\n",
    "    total_epoch_loss_train = train(svi, data_loaders[\"train\"], use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    if VERBOSE:\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, data_loaders[\"test\"], use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        if VERBOSE:\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reconstruction accuracy of trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557ad59e5a07470bb7f2c86c11998509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='data id', max=256), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(data_loaders[\"train\"])\n",
    "xs, ys = next(data_iter)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cuda()\n",
    "    ys = ys.cuda()\n",
    "rs = vae.reconstruct_image(xs, ys)\n",
    "\n",
    "def f(i):\n",
    "    compare_reconstruction(xs[i], rs[i])\n",
    "    \n",
    "interact(f, i=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0,description='data id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Structural Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCM():\n",
    "    def __init__(self, vae, mu, sigma):\n",
    "        self.vae = vae\n",
    "        self.image_dim = vae.image_dim\n",
    "        self.z_dim = vae.z_dim\n",
    "        \n",
    "        # these are used for f_X\n",
    "        self.label_dims = vae.label_shape\n",
    "        \n",
    "        def f_X(Y, Z, N):\n",
    "            zs = Z.cuda()\n",
    "            ys = torch.tensor(Y).reshape(1,-1).cuda()\n",
    "            p = vae.decoder.forward(zs, ys)\n",
    "            return (N < p.cpu()).type(torch.float)\n",
    "        \n",
    "        def f_Y(N):\n",
    "            return 1.0*N\n",
    "        \n",
    "        def f_Z(N):\n",
    "            return N * sigma + mu\n",
    "        \n",
    "        def model(noise): \n",
    "            N_X = pyro.sample( 'N_X', noise['N_X'].to_event(1) )\n",
    "            # denoted using the index in the sequence \n",
    "            # that they are stored in as vae.label_names:\n",
    "            # ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "            N_Y_1 = pyro.sample( 'N_Y_1', noise['N_Y_1'] )\n",
    "            N_Y_2 = pyro.sample( 'N_Y_2', noise['N_Y_2'] )\n",
    "            N_Y_3 = pyro.sample( 'N_Y_3', noise['N_Y_3'])\n",
    "            N_Y_4 = pyro.sample( 'N_Y_4', noise['N_Y_4'])\n",
    "            N_Y_5 = pyro.sample( 'N_Y_5', noise['N_Y_5'])\n",
    "            N_Z = pyro.sample( 'N_Z', noise['N_Z'].to_event(1) )\n",
    "            \n",
    "            Y_1 = pyro.sample('Y_1', dist.Normal( f_Y(N_Y_1), 1e-2) )\n",
    "            Y_2 = pyro.sample('Y_2', dist.Normal( f_Y(N_Y_2), 1e-2) )\n",
    "            Y_3 = pyro.sample('Y_3', dist.Normal( f_Y(N_Y_3), 1e-2) )\n",
    "            Y_4 = pyro.sample('Y_4', dist.Normal( f_Y(N_Y_4), 1e-2) )\n",
    "            Y_5 = pyro.sample('Y_5', dist.Normal( f_Y(N_Y_5), 1e-2) )\n",
    "\n",
    "            Z = pyro.sample(\"Z\", dist.Normal( f_Z( N_Z ), 1e-1).to_event(1) )\n",
    "            X = pyro.sample('X', dist.Normal( f_X( (Y_1,Y_2,Y_3,Y_4,Y_5), Z, N_X ), 1e-2).to_event(1))\n",
    "\n",
    "            noise_samples = N_X, (N_Y_1, N_Y_2, N_Y_3, N_Y_4, N_Y_5), N_Z\n",
    "            variable_samples = X, (Y_1, Y_2, Y_3, Y_4, Y_5), Z\n",
    "\n",
    "            return variable_samples, noise_samples\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.init_noise = {\n",
    "            'N_X'   : dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)),\n",
    "            'N_Z'   : dist.Normal(torch.zeros(vae.z_dim), torch.ones(vae.z_dim)),\n",
    "            'N_Y_1' : dist.Uniform(torch.tensor(0.), torch.tensor(2.)),\n",
    "            'N_Y_2' : dist.Uniform(torch.tensor(.5), torch.tensor(1.)),\n",
    "            'N_Y_3' : dist.Uniform(torch.tensor(0.), torch.tensor(2*np.pi)),\n",
    "            'N_Y_4' : dist.Uniform(torch.tensor(0.), torch.tensor(1.)),\n",
    "            'N_Y_5' : dist.Uniform(torch.tensor(0.), torch.tensor(1.))\n",
    "        }\n",
    "        \n",
    "    def update_noise_svi(self, obs_data):\n",
    "        # assume all noise variables are normal distributions\n",
    "        # use svi to find out the mu, sigma of the distributions\n",
    "        # for the condition outlined in obs_data\n",
    "        \n",
    "        def guide(noise):\n",
    "            # create params with constraints\n",
    "\n",
    "            los = {\"N_Y_1\": 0., \"N_Y_2\": 0.5, \"N_Y_3\": 0., \"N_Y_4\": 0., \"N_Y_5\":0.}\n",
    "            his = {\"N_Y_1\": 2., \"N_Y_2\": 1., \"N_Y_3\": 2*np.pi, \"N_Y_4\": 1., \"N_Y_5\":1.}\n",
    "            for noise_term in noise.keys():\n",
    "                if noise_term == \"N_Z\":\n",
    "                    mu = pyro.param(\"N_Z_mu\", torch.zeros(self.z_dim), constraint=constraints.interval(-3., 3.))\n",
    "                    sigma = pyro.param(\"N_Z_sigma\", 0.1*torch.ones(self.z_dim), constraint=constraints.interval(0.001, 1.))\n",
    "                    pyro.sample(\"N_Z\", dist.Uniform(mu, sigma).to_event(1))\n",
    "                elif noise_term == \"N_X\":\n",
    "                    lo = pyro.param(\"N_X_lo\", torch.zeros(self.image_dim), constraint=constraints.interval(0., 1.))\n",
    "                    hi = pyro.param(\"N_X_hi\", torch.ones(self.image_dim), constraint=constraints.interval(0., 1.))\n",
    "                    pyro.sample(\"N_X\", dist.Uniform(lo, hi).to_event(1))\n",
    "                else:\n",
    "                    lo = pyro.param(noise_term + \"_lo\", \n",
    "                                    torch.tensor(los[noise_term]+1e-1), \n",
    "                                    constraint=constraints.interval(los[noise_term], his[noise_term]))\n",
    "                    hi = pyro.param(noise_term + \"_hi\", \n",
    "                                    torch.tensor(his[noise_term]-1e-1), \n",
    "                                    constraint=constraints.interval(los[noise_term], his[noise_term]))\n",
    "                    pyro.sample(noise_term, dist.Uniform(lo, hi))\n",
    "                 \n",
    "    \n",
    "        obs_model = pyro.condition(self.model, obs_data)\n",
    "        \n",
    "        pyro.clear_param_store()\n",
    "        svi = SVI(\n",
    "            model= obs_model,\n",
    "            guide= guide,\n",
    "            optim= SGD({\"lr\": 1e-4, 'momentum': 0.1}),\n",
    "            loss=Trace_ELBO(retain_graph=True)\n",
    "        )\n",
    "        \n",
    "        num_steps = 1000\n",
    "        samples = defaultdict(list)\n",
    "        for t in range(num_steps):\n",
    "            loss = svi.step(self.init_noise)\n",
    "            if t % 100 == 0:\n",
    "                print(\"step %d: loss of %.2f\" % (t, loss))\n",
    "#         for noise in self.init_noise.keys():\n",
    "#             mu = '{}_mu'.format(noise)\n",
    "#             sigma = '{}_sigma'.format(noise)\n",
    "#             samples[mu].append(pyro.param(mu).detach().numpy())\n",
    "#             samples[sigma].append(pyro.param(sigma).detach().numpy())\n",
    "#         means = {k: torch.tensor(np.array(v).mean(axis=0)) for k, v in samples.items()}\n",
    "        \n",
    "        updated_noise = {\n",
    "            'N_X'  : dist.Uniform(pyro.param(\"N_X_lo\").detach(), pyro.param(\"N_X_hi\").detach()),\n",
    "            'N_Z'  : dist.Normal(pyro.param(\"N_Z_mu\").detach(), pyro.param(\"N_Z_sigma\").detach()),\n",
    "            'N_Y_1': dist.Uniform(pyro.param(\"N_Y_1_lo\").detach(),pyro.param(\"N_Y_1_hi\").detach()),\n",
    "            'N_Y_2': dist.Uniform(pyro.param(\"N_Y_2_lo\").detach(),pyro.param(\"N_Y_2_hi\").detach()),\n",
    "            'N_Y_3': dist.Uniform(pyro.param(\"N_Y_3_lo\").detach(),pyro.param(\"N_Y_3_hi\").detach()),\n",
    "            'N_Y_4': dist.Uniform(pyro.param(\"N_Y_4_lo\").detach(),pyro.param(\"N_Y_4_hi\").detach()),\n",
    "            'N_Y_5': dist.Uniform(pyro.param(\"N_Y_5_lo\").detach(),pyro.param(\"N_Y_5_hi\").detach())\n",
    "        }\n",
    "        \n",
    "        return updated_noise\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.model(self.init_noise)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob at site 'N_Y':\nThe value argument must be within the support\nTrace Shapes:  \n Param Sites:  \nSample Sites:  \n     N_Y dist |\n        value |",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/torch/distributions/uniform.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument must be within the support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument must be within the support",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-e36ae2ca7882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mscm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mcond_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_noise_svi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"N_Y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-141-e36ae2ca7882>\u001b[0m in \u001b[0;36mupdate_noise_svi\u001b[0;34m(self, data, noise)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step %d: loss of %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 52\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_subsample_sites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                     ValueError(\"Error while computing log_prob at site '{}':\\n{}\\n{}\"\n\u001b[1;32m    222\u001b[0m                                                .format(name, exc_value, shapes)),\n\u001b[0;32m--> 223\u001b[0;31m                                     traceback)\n\u001b[0m\u001b[1;32m    224\u001b[0m                     \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unscaled_log_prob\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"log_prob\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/torch/distributions/uniform.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument must be within the support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_checked_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error while computing log_prob at site 'N_Y':\nThe value argument must be within the support\nTrace Shapes:  \n Param Sites:  \nSample Sites:  \n     N_Y dist |\n        value |"
     ]
    }
   ],
   "source": [
    "class SCM2():\n",
    "    def __init__(self):\n",
    "        def f_Y(N):\n",
    "            return N\n",
    "        \n",
    "        def model(noise):\n",
    "            N_Y = pyro.sample(\"N_Y\", noise[\"N_Y\"])\n",
    "            Y = pyro.sample(\"Y\", dist.Normal(f_Y(N_Y), 1e-1))\n",
    "            return Y\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def update_noise_svi(self, data, noise):\n",
    "        def guide(noise):\n",
    "            mu = pyro.param(\"N_Y_mu\", torch.tensor(0.), constraint = constraints.interval(-1.,1.))\n",
    "            sigma = pyro.param(\"N_Y_sigma\", torch.tensor(0.1), constraint = constraints.interval(0.001,1.))\n",
    "            pyro.sample(\"N_Y\", dist.Normal(mu,  sigma))\n",
    "            \n",
    "        obs_model = pyro.condition(self.model, data)\n",
    "        pyro.clear_param_store()\n",
    "        svi = SVI(\n",
    "            model=obs_model,\n",
    "            guide=guide,\n",
    "            optim=SGD({\"lr\": 0.001, 'momentum': 0.1}),\n",
    "            loss=Trace_ELBO()\n",
    "        )\n",
    "        mus = []\n",
    "        sigmas = []\n",
    "        for t in range(10000):\n",
    "            loss = svi.step(noise)\n",
    "            if t % 1000 == 0:\n",
    "                print(\"step %d: loss of %.2f\" % (t, loss))\n",
    "                mus.append(pyro.param(\"N_Y_mu\").detach().numpy())\n",
    "                sigmas.append(pyro.param(\"N_Y_sigma\").detach().numpy())\n",
    "        updated_noise = {\n",
    "            'N_Y'  : dist.Normal(torch.tensor(np.mean(mus, axis=0)), torch.tensor(np.mean(sigmas, axis=0)))\n",
    "        }\n",
    "        return updated_noise\n",
    "\n",
    "scm2 = SCM2()\n",
    "cond_data = {\"Y\": torch.tensor(0.1)}\n",
    "print(scm2.update_noise_svi(cond_data, {\"N_Y\": dist.Uniform(torch.tensor(0.), torch.tensor(1.))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing scm works\n",
    "ox, y = get_specific_data(cuda=True)\n",
    "mu, sigma = vae.encoder.forward(ox,vae.remap_y(y))\n",
    "scm = SCM(vae, mu.cpu(), sigma.cpu())\n",
    "(rx,ry,z),_ = scm()\n",
    "plot_image(rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss of -8298.89\n",
      "step 100: loss of -12388.56\n",
      "step 200: loss of -12922.23\n",
      "step 300: loss of -12924.24\n",
      "step 400: loss of -12947.31\n",
      "step 500: loss of -13128.72\n",
      "step 600: loss of -12995.28\n",
      "step 700: loss of -13026.43\n",
      "step 800: loss of -12997.69\n",
      "step 900: loss of -12971.08\n",
      "{'N_X': Uniform(low: torch.Size([4096]), high: torch.Size([4096])), 'N_Z': Normal(loc: torch.Size([50]), scale: torch.Size([50])), 'N_Y_1': Uniform(low: 0.9808346629142761, high: 1.0185065269470215), 'N_Y_2': Uniform(low: 0.5217043161392212, high: 0.5357774496078491), 'N_Y_3': Uniform(low: 0.33903664350509644, high: 0.37028399109840393), 'N_Y_4': Uniform(low: 0.959896445274353, high: 0.9771520495414734), 'N_Y_5': Uniform(low: 0.09888982772827148, high: 0.9011101126670837)}\n"
     ]
    }
   ],
   "source": [
    "cond_data = {}\n",
    "for i in range(0, 5):\n",
    "    cond_data[\"Y_{}\".format(i)] = torch.tensor(y[0,i].cpu()).to(torch.float32)\n",
    "\n",
    "cond_noise = scm.update_noise_svi(cond_data)\n",
    "\n",
    "print(cond_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "(rx,ry,z),_ = scm.model(cond_noise)\n",
    "plot_image(rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rx,_,_), _ = scm.model(cond_noise)\n",
    "compare_reconstruction(ox, rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervened = pyro.do(scm.model, data={\"Y_1\": torch.tensor(1.)})\n",
    "cond_data = {}\n",
    "for i in range(1, 6):\n",
    "    cond_data[\"Y_{}\".format(i)] = torch.tensor(y[0,i].cpu()).to(torch.float32)\n",
    "conditioned_model = pyro.condition(scm.model, data=cond_data)\n",
    "cond_noise = scm.update_noise_svi(cond_data)\n",
    "print(cond_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1, _, _), _ = intervened(cond_noise)\n",
    "\n",
    "compare_reconstruction(ox, x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check that do-operation should work on trained network\n",
    "\n",
    "At one point I was concerned that the network was encoding too much information in the latent encoding z and thus conditioning on the labels wasnt enough to change the output.  According to the results below, that is not the case.  We should be able to condition on the labels and get sensible outputs from the conditioned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, y_original = get_specific_data(cuda=True)\n",
    "print('top: ',y_original)\n",
    "mu, sigma = vae.encoder.forward(original,vae.remap_y(y_original))\n",
    "B = 100\n",
    "zs = torch.cat([dist.Normal(mu.cpu(), sigma.cpu()).sample() for a in range(B)], 0)\n",
    "ys = torch.cat([vae.remap_y(y_original) for a in range(B)], 0)\n",
    "rs = vae.decoder.forward(zs.cuda(), ys).detach()\n",
    "compare_to_density(original,rs)\n",
    "\n",
    "y_new = torch.tensor(y_original)\n",
    "y_new[0,1] = (y_original[0,1] + 1) % 2\n",
    "print('bottom: ', y_new)\n",
    "zs = torch.cat([dist.Normal(mu.cpu(), sigma.cpu()).sample() for a in range(B)], 0)\n",
    "ys = torch.cat([vae.remap_y(y_new) for a in range(B)], 0)\n",
    "rs = vae.decoder.forward(zs.cuda(), ys).detach()\n",
    "compare_to_density(original,rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
