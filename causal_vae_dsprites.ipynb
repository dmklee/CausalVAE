{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.z_dim = z_dim\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc31 = nn.Linear(1000, z_dim)  # mu values\n",
    "        self.fc32 = nn.Linear(1000, z_dim)  # sigma values\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        # define the forward computation on the image xs and label ys\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        xs = xs.reshape(-1, self.image_dim)\n",
    "        #now concatenate the image and label\n",
    "        inputs = torch.cat((xs,ys), -1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc31(hidden2)\n",
    "        z_scale = torch.exp(self.fc32(hidden2))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        hidden_dim = 1000\n",
    "        self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, image_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, zs, ys):\n",
    "        # define the forward computation on the latent z and label y\n",
    "        # first concatenate z and y\n",
    "        inputs = torch.cat((zs, ys),-1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        hidden3 = self.softplus(self.fc3(hidden2))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc4(hidden3))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.image_dim = 64**2\n",
    "        self.label_shape = np.array((1,3,6,40,32,32))\n",
    "        self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))\n",
    "        self.label_dim = np.sum(self.label_shape)\n",
    "        self.z_dim = 50                                    \n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the paramters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # dsprites label (latent)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"cvae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "    \n",
    "            loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label \n",
    "        loc, scale are given by a neural network `encoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample (and score) the latent handwriting-style with the variational\n",
    "            # distribution q(z|x) = normal(loc(x),scale(x))\n",
    "    \n",
    "            loc, scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "            \n",
    "    def remap_y(self, ys):\n",
    "        new_ys = []\n",
    "        options = dict(dtype=ys.dtype, device=ys.device)\n",
    "        for i, label_length in enumerate(self.label_shape):\n",
    "            prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)\n",
    "            new_ys.append(pyro.sample(\"y_%s\" % self.label_names[i], dist.OneHotCategorical(prior), \n",
    "                                   obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))\n",
    "        new_ys = torch.cat(new_ys, -1)\n",
    "        return new_ys.to(torch.float32)\n",
    "            \n",
    "    def reconstruct_image(self, xs, ys):\n",
    "        # backward\n",
    "        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
    "        # forward\n",
    "        loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "        return dist.Bernoulli(loc).to_event(1).sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):\n",
    "    train_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(train_y.astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    test_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(test_y.astype(np.float32))\n",
    "    )    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\"train\":train_loader, \"test\":test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip = np.load(\n",
    "    'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
    "    encoding = 'bytes',\n",
    "    allow_pickle=True\n",
    ")\n",
    "imgs = dataset_zip['imgs']\n",
    "labels = dataset_zip['latents_classes']\n",
    "label_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
    "label_names = dataset_zip['metadata'][()][b'latents_names']\n",
    "\n",
    "# Sample imgs randomly\n",
    "indices_sampled = np.arange(imgs.shape[0])\n",
    "np.random.shuffle(indices_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "labels_sampled = labels[indices_sampled]\n",
    "\n",
    "data_loaders = setup_data_loaders(\n",
    "    imgs_sampled[1000:],\n",
    "    imgs_sampled[:1000],\n",
    "    labels_sampled[1000:],\n",
    "    labels_sampled[:1000],\n",
    "    batch_size=256,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_specific_data(args=dict(), cuda=False):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        cuda - bool to indicate whether the output should be placed on GPU\n",
    "    '''\n",
    "    names_dict = {'shape': 1, 'scale': 2, 'orientation': 3, 'posX': 4, 'posY': 5}\n",
    "    selected_ind = np.ones(imgs.shape[0], dtype=bool)\n",
    "    for k,v in args.items():\n",
    "        col_id = names_dict[k]\n",
    "        selected_ind = np.bitwise_and(selected_ind, labels[:, col_id] == v)\n",
    "    ind = np.random.choice(np.arange(imgs.shape[0])[selected_ind])\n",
    "    x = torch.from_numpy(imgs[ind].reshape(1,64**2).astype(np.float32))\n",
    "    y = torch.from_numpy(labels[ind].reshape(1,6).astype(np.float32))\n",
    "    if not cuda:\n",
    "        return x,y\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    return x,y\n",
    "\n",
    "def see_specific_image(args=dict(), verbose=True):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        verbose - bool to indicate whether the full class label should be written \n",
    "                    as the title of the plot\n",
    "    '''\n",
    "    x,y = get_specific_data(args, cuda=False)\n",
    "    plt.figure()\n",
    "    plt.imshow(x.reshape(64,64), interpolation='nearest', cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "    if verbose:\n",
    "        string = ''\n",
    "        for i, s in enumerate(['Shape', 'Scale', 'Orientation', 'PosX', 'PosY']):\n",
    "            string += '%s: %d, ' % (s, int(y[0][i+1]))\n",
    "            if i == 2:\n",
    "                string = string[:-2] + '\\n'\n",
    "        plt.title(string[:-2])\n",
    "        \n",
    "see_specific_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3ab5c512644257a6fcf2ba52f31881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='shape', max=2), IntSlider(value=0, description='scale', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.find_in_dataset(shape, scale, orient, posX, posY)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_names = ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "y_shapes = np.array((3,6,40,32,32))\n",
    "img_dict = {}\n",
    "for i, img in enumerate(imgs_sampled):\n",
    "    img_dict[tuple(labels_sampled[i])] = img\n",
    "    \n",
    "def find_in_dataset(shape, scale, orient, posX, posY):\n",
    "    fig = plt.figure()\n",
    "    img = img_dict[(0, shape, scale, orient, posX, posY)]\n",
    "    plt.imshow(img.reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(find_in_dataset, shape=widgets.IntSlider(min=0, max=2, step=1, value=0),\n",
    "                          scale=widgets.IntSlider(min=0, max=5, step=1, value=0),\n",
    "                            orient=widgets.IntSlider(min=0, max=39, step=1, value=0),\n",
    "                            posX=widgets.IntSlider(min=0, max=31, step=1, value=0),\n",
    "                            posY=widgets.IntSlider(min=0, max=31, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for xs,ys in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(xs, ys)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for xs, ys in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(xs, ys)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 0\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### FOR SAVING AND LOADING MODEL\n",
    "################################\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "PATH = \"trained_model.save\"\n",
    "\n",
    "# new model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# save current model\n",
    "# torch.save(vae.state_dict(), PATH)\n",
    "\n",
    "# to load params from trained model\n",
    "vae = CVAE(use_cuda=USE_CUDA)\n",
    "vae.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935fefbb79a14937bf871bd1a5a0bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "\n",
    "VERBOSE = True\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in pbar:\n",
    "    total_epoch_loss_train = train(svi, data_loaders[\"train\"], use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    if VERBOSE:\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, data_loaders[\"test\"], use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        if VERBOSE:\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the reconstruction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loaders[\"train\"])\n",
    "xs, ys = next(data_iter)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cuda()\n",
    "    ys = ys.cuda()\n",
    "rs = vae.reconstruct_image(xs, ys)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cpu()\n",
    "    rs = rs.cpu()\n",
    "originals = xs.numpy().reshape(-1, 64,64)\n",
    "recons = rs.numpy().reshape(-1,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAADMCAYAAAB+36QhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA7hJREFUeJzt3UFO5DAQQFGMuP+VPSuYkQY1NEpw4v/eGjVZlPRdMaLHnPMFgK7X1Q8AwFpCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAEPe2+gF+YozhP+VxqjnnWPF7zTZn+2y2bQQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQJwQAcUIAECcEAHFCABAnBABxQgAQ97b6AVhjzvnUz48xTnoSOJbZfp6NACBOCADivBoKeHZV/uozrNJchdk+ho0AIM5GsLEjTkuPPrd6emI9s30sGwFAnI1gQ2edlmA1s30OGwFAnBAAxHk1tAkrM7sy2+ezEQDE2QhuzmmJXZnt32MjAIizEdyU0xK7Mtu/z0YAECcEAHFCABAnBABxLotvxCUauzLba9kIAOKEACBOCADihAAgzmXxDbhIY1dm+xpsBABxNgKeVvtibzqqs20jAIgTAoA4r4YuyiUauzLb12MjAIgTAoA4IQCIEwKAOJfFfEv176vZn9m2EQDkCQFAnBAAxLkj4CHvT9mV2f7LRgAQJwQAcV4N8R8rM7sy25+zEQDECQFAnBAAxAkBQJzLYj64SGNXZvsxGwFAnI0ApyW2Zba/x0YAEGcjuKh/TzJHftm3ExKrme3rsREAxAkBQJxXQzfwvvL+dI22MnNVZvsabAQAcTaCG3H6YVdmey0bAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQNyYc65+BgAWshEAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQJwQAMQJAUCcEADECQFAnBAAxAkBQNwf43xjuWFT88AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = xs[0].cuda()\n",
    "y = ys[0]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121)\n",
    "plt.imshow(x.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "plt.axis('off')\n",
    "ax1 = fig.add_subplot(122)\n",
    "plt.imshow(x.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd104b11bc74118b5c943e0feab5cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', max=256), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(x)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(originals[x], cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(recons[x], cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(f, x=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCM(vae, xs):\n",
    "    z_dim = vae.z_dim\n",
    "    Nx = pyro.sample(\"Nx\", dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)))\n",
    "    Nz = pyro.sample(\"Nz\", dist.Normal(torch.zeros(z_dim), torch.ones(z_dim)))\n",
    "    Ny = []\n",
    "    Y = []\n",
    "    ys = []\n",
    "    m = torch.distributions.gumbel.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n",
    "    for label_id in range(6):\n",
    "        name = vae.label_names[label_id]\n",
    "        length = vae.label_shape[label_id]\n",
    "        new = pyro.sample(\"Ny_%s\"%name, dist.Uniform(torch.zeros(length), torch.ones(length)) )\n",
    "        Ny.append(new)\n",
    "        gumbel_vars = torch.tensor([m.sample() for _ in range(length)])\n",
    "        max_ind = torch.argmax(torch.log(new) + gumbel_vars).item()\n",
    "        Y.append(pyro.sample(\"Y_%s\"%name, dist.Normal(torch.tensor(max_ind*1.0), 1e-4)))\n",
    "        ys.append(torch.nn.functional.one_hot(torch.tensor(max_ind), int(length)))\n",
    "                 \n",
    "    Y = torch.tensor(Y)\n",
    "    ys = torch.cat(ys).to(torch.float32).reshape(1,-1).cuda()\n",
    "    mu, sigma = vae.encoder.forward(xs, ys)\n",
    "    Z = pyro.sample(\"Z\", dist.Normal(mu.cpu() + Nz*sigma.cpu(), 1e-4))\n",
    "    zs = Z.cuda()\n",
    "    p = vae.decoder.forward(zs,ys).cpu()\n",
    "    X = pyro.sample(\"X\", dist.Normal((Nx < p).to(torch.float), 1e-4))\n",
    "    return X, Y, Z\n",
    "\n",
    "def generate_y_dict(label_names, y):\n",
    "    data = dict()\n",
    "    for i, name in enumerate(label_names):\n",
    "        data[\"Y_\"+name] = int(y[i]) \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = next(data_iter)\n",
    "x = xs[0].reshape(1,-1).cuda()\n",
    "y = ys[0].reshape(1,-1).cuda()\n",
    "\n",
    "x_, y_, z_ = SCM(vae, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  4., 19., 22., 24.]], device='cuda:0')\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n"
     ]
    }
   ],
   "source": [
    "data = generate_y_dict(vae.label_names, y.cpu()[0])\n",
    "cond_model = pyro.condition(SCM, data=data)\n",
    "print(y)\n",
    "N = 5\n",
    "result = np.zeros((100,64**2))\n",
    "for i in range(N):\n",
    "    ix,iy,_ = cond_model(vae, x)\n",
    "    result[i] = ix.cpu()\n",
    "    print(iy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.importance import Importance\n",
    "from pyro.infer.mcmc import MCMC\n",
    "from pyro.infer.mcmc.nuts import HMC\n",
    "\n",
    "intervened_model = pyro.do(SCM, data={\"Y_shape\": torch.tensor(0.)})\n",
    "conditioned_model = pyro.condition(SCM, data={\"Y_shape\": torch.tensor(1.0)})\n",
    "\n",
    "posterior = pyro.infer.Importance(conditioned_model, num_samples = 1000)\n",
    "posterior.run(vae, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posterior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-591b88651e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnoise_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Ny_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Nx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Nz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'posterior' is not defined"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "result = np.zeros((N, vae.image_dim))\n",
    "for i in range(N):\n",
    "    trace = posterior()\n",
    "    data = dict()\n",
    "    noise_vars = ['Ny_' + s for s in vae.label_names] + ['Nx', 'Nz']\n",
    "    for name in noise_vars:\n",
    "        data[name] = trace.nodes[name]['value']\n",
    "        \n",
    "    con_obj = pyro.condition(intervened_model, data = data)\n",
    "    result[i] = con_obj(vae, x)[0]\n",
    "    \n",
    "# now generate density image\n",
    "density_comp(x, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_map(img):\n",
    "    dens_img = np.mean(img, axis=0).reshape(64,64)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(dens_img, interpolation='nearest', cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "def density_comp(img, recons):\n",
    "    dens_img = np.mean(recons, axis=0).reshape(64,64)\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(121)\n",
    "    plt.imshow(img.cpu().reshape(64,64), interpolation='nearest', cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig.add_subplot(122)\n",
    "    plt.imshow(dens_img, interpolation='nearest', cmap=\"Greys_r\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
