{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.z_dim = z_dim\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc31 = nn.Linear(1000, z_dim)  # mu values\n",
    "        self.fc32 = nn.Linear(1000, z_dim)  # sigma values\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        # define the forward computation on the image xs and label ys\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        xs = xs.reshape(-1, self.image_dim)\n",
    "        #now concatenate the image and label\n",
    "        inputs = torch.cat((xs,ys), -1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc31(hidden2)\n",
    "        z_scale = torch.exp(self.fc32(hidden2))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        hidden_dim = 1000\n",
    "        self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, image_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, zs, ys):\n",
    "        # define the forward computation on the latent z and label y\n",
    "        # first concatenate z and y\n",
    "        inputs = torch.cat((zs, ys),-1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        hidden3 = self.softplus(self.fc3(hidden2))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc4(hidden3))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.image_dim = 64**2\n",
    "        self.label_shape = np.array((1,3,6,40,32,32))\n",
    "        self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))\n",
    "        self.label_dim = np.sum(self.label_shape)\n",
    "        self.z_dim = 50                                    \n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the paramters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # dsprites label (latent)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"cvae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "    \n",
    "            loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label \n",
    "        loc, scale are given by a neural network `encoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample (and score) the latent handwriting-style with the variational\n",
    "            # distribution q(z|x) = normal(loc(x),scale(x))\n",
    "    \n",
    "            loc, scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "            \n",
    "    def remap_y(self, ys):\n",
    "        new_ys = []\n",
    "        options = dict(dtype=ys.dtype, device=ys.device)\n",
    "        for i, label_length in enumerate(self.label_shape):\n",
    "            prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)\n",
    "            new_ys.append(pyro.sample(\"y_%s\" % self.label_names[i], dist.OneHotCategorical(prior), \n",
    "                                   obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))\n",
    "        new_ys = torch.cat(new_ys, -1)\n",
    "        return new_ys.to(torch.float32)\n",
    "            \n",
    "    def reconstruct_image(self, xs, ys):\n",
    "        # backward\n",
    "        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
    "        # forward\n",
    "        loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "        return dist.Bernoulli(loc).to_event(1).sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):\n",
    "    train_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(train_y.astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    test_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(test_y.astype(np.float32))\n",
    "    )    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\"train\":train_loader, \"test\":test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip = np.load(\n",
    "    'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
    "    encoding = 'bytes',\n",
    "    allow_pickle=True\n",
    ")\n",
    "imgs = dataset_zip['imgs']\n",
    "labels = dataset_zip['latents_classes']\n",
    "label_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
    "label_names = dataset_zip['metadata'][()][b'latents_names']\n",
    "\n",
    "# Sample imgs randomly\n",
    "indices_sampled = np.arange(imgs.shape[0])\n",
    "np.random.shuffle(indices_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "labels_sampled = labels[indices_sampled]\n",
    "\n",
    "data_loaders = setup_data_loaders(\n",
    "    imgs_sampled[1000:],\n",
    "    imgs_sampled[:1000],\n",
    "    labels_sampled[1000:],\n",
    "    labels_sampled[:1000],\n",
    "    batch_size=256,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ae45f1988e46e19ccc4b27f64dc946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='shape', max=2), IntSlider(value=4, description='scale', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.find_in_dataset(shape, scale, orient, posX, posY)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_names = ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "y_shapes = np.array((3,6,40,32,32))\n",
    "img_dict = {}\n",
    "for i, img in enumerate(imgs_sampled):\n",
    "    img_dict[tuple(labels_sampled[i])] = img\n",
    "    \n",
    "def find_in_dataset(shape, scale, orient, posX, posY):\n",
    "    fig = plt.figure()\n",
    "    img = img_dict[(0, shape, scale, orient, posX, posY)]\n",
    "    plt.imshow(img.reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(find_in_dataset, shape=widgets.IntSlider(min=0, max=2, step=1, value=npr.randint(2)),\n",
    "                          scale=widgets.IntSlider(min=0, max=5, step=1, value=npr.randint(5)),\n",
    "                            orient=widgets.IntSlider(min=0, max=39, step=1, value=npr.randint(39)),\n",
    "                            posX=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)),\n",
    "                            posY=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_specific_data(args=dict(), cuda=False):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        cuda - bool to indicate whether the output should be placed on GPU\n",
    "    '''\n",
    "    names_dict = {'shape': 1, 'scale': 2, 'orientation': 3, 'posX': 4, 'posY': 5}\n",
    "    selected_ind = np.ones(imgs.shape[0], dtype=bool)\n",
    "    for k,v in args.items():\n",
    "        col_id = names_dict[k]\n",
    "        selected_ind = np.bitwise_and(selected_ind, labels[:, col_id] == v)\n",
    "    ind = np.random.choice(np.arange(imgs.shape[0])[selected_ind])\n",
    "    x = torch.from_numpy(imgs[ind].reshape(1,64**2).astype(np.float32))\n",
    "    y = torch.from_numpy(labels[ind].reshape(1,6).astype(np.float32))\n",
    "    if not cuda:\n",
    "        return x,y\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    return x,y\n",
    "\n",
    "def see_specific_image(args=dict(), verbose=True):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        verbose - bool to indicate whether the full class label should be written \n",
    "                    as the title of the plot\n",
    "    '''\n",
    "    x,y = get_specific_data(args, cuda=False)\n",
    "    plt.figure()\n",
    "    plt.imshow(x.reshape(64,64), interpolation='nearest', cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "    if verbose:\n",
    "        string = ''\n",
    "        for i, s in enumerate(['Shape', 'Scale', 'Orientation', 'PosX', 'PosY']):\n",
    "            string += '%s: %d, ' % (s, int(y[0][i+1]))\n",
    "            if i == 2:\n",
    "                string = string[:-2] + '\\n'\n",
    "        plt.title(string[:-2])\n",
    "        \n",
    "def compare_reconstruction(original, recon):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array for recon image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(recon.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstruction')\n",
    "    \n",
    "def compare_to_density(original, recons):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array of multiple recon images\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(torch.mean(recons.cpu(), 0).reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstructions')\n",
    "\n",
    "        \n",
    "see_specific_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training or Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for xs,ys in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(xs, ys)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for xs, ys in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(xs, ys)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 0\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### FOR SAVING AND LOADING MODEL\n",
    "################################\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "PATH = \"trained_model.save\"\n",
    "\n",
    "# new model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# save current model\n",
    "# torch.save(vae.state_dict(), PATH)\n",
    "\n",
    "# to load params from trained model\n",
    "vae = CVAE(use_cuda=USE_CUDA)\n",
    "vae.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935fefbb79a14937bf871bd1a5a0bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "\n",
    "VERBOSE = True\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in pbar:\n",
    "    total_epoch_loss_train = train(svi, data_loaders[\"train\"], use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    if VERBOSE:\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, data_loaders[\"test\"], use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        if VERBOSE:\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the reconstruction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c71e47ca164895b547e33f005f85e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='data id', max=256), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(data_loaders[\"train\"])\n",
    "xs, ys = next(data_iter)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cuda()\n",
    "    ys = ys.cuda()\n",
    "rs = vae.reconstruct_image(xs, ys)\n",
    "\n",
    "def f(i):\n",
    "    compare_reconstruction(xs[i], rs[i])\n",
    "    \n",
    "interact(f, i=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0,description='data id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCM(vae, xs):\n",
    "    z_dim = vae.z_dim\n",
    "    Nx = pyro.sample(\"Nx\", dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)))\n",
    "    Nz = pyro.sample(\"Nz\", dist.Normal(torch.zeros(z_dim), torch.ones(z_dim)))\n",
    "    Ny = []\n",
    "    Y = []\n",
    "    ys = []\n",
    "    m = torch.distributions.gumbel.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n",
    "    for label_id in range(6):\n",
    "        name = vae.label_names[label_id]\n",
    "        length = vae.label_shape[label_id]\n",
    "        new = pyro.sample(\"Ny_%s\"%name, dist.Uniform(torch.zeros(length), torch.ones(length)) )\n",
    "        Ny.append(new)\n",
    "        gumbel_vars = torch.tensor([m.sample() for _ in range(length)])\n",
    "        max_ind = torch.argmax(torch.log(new) + gumbel_vars).item()\n",
    "        Y.append(pyro.sample(\"Y_%s\"%name, dist.Normal(torch.tensor(max_ind*1.0), 1e-4)))\n",
    "        ys.append(torch.nn.functional.one_hot(torch.tensor(max_ind), int(length)))\n",
    "                 \n",
    "    Y = torch.tensor(Y)\n",
    "    ys = torch.cat(ys).to(torch.float32).reshape(1,-1).cuda()\n",
    "    mu, sigma = vae.encoder.forward(xs, ys)\n",
    "    Z = pyro.sample(\"Z\", dist.Normal(mu.cpu() + Nz*sigma.cpu(), 1e-4))\n",
    "    zs = Z.cuda()\n",
    "    p = vae.decoder.forward(zs,ys).cpu()\n",
    "    X = pyro.sample(\"X\", dist.Normal((Nx < p).to(torch.float), 1e-4))\n",
    "    return X, Y, Z\n",
    "\n",
    "def generate_y_dict(label_names, y):\n",
    "    data = dict()\n",
    "    for i, name in enumerate(label_names):\n",
    "        data[\"Y_\"+name] = int(y[i]) \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = next(data_iter)\n",
    "x = xs[0].reshape(1,-1).cuda()\n",
    "y = ys[0].reshape(1,-1).cuda()\n",
    "\n",
    "x_, y_, z_ = SCM(vae, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  4., 19., 22., 24.]], device='cuda:0')\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n",
      "tensor([ 0,  0,  4, 19, 22, 24])\n"
     ]
    }
   ],
   "source": [
    "data = generate_y_dict(vae.label_names, y.cpu()[0])\n",
    "cond_model = pyro.condition(SCM, data=data)\n",
    "print(y)\n",
    "N = 5\n",
    "result = np.zeros((100,64**2))\n",
    "for i in range(N):\n",
    "    ix,iy,_ = cond_model(vae, x)\n",
    "    result[i] = ix.cpu()\n",
    "    print(iy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.importance import Importance\n",
    "from pyro.infer.mcmc import MCMC\n",
    "from pyro.infer.mcmc.nuts import HMC\n",
    "\n",
    "intervened_model = pyro.do(SCM, data={\"Y_shape\": torch.tensor(0.)})\n",
    "conditioned_model = pyro.condition(SCM, data={\"Y_shape\": torch.tensor(1.0)})\n",
    "\n",
    "posterior = pyro.infer.Importance(conditioned_model, num_samples = 1000)\n",
    "posterior.run(vae, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posterior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-591b88651e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnoise_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Ny_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Nx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Nz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'posterior' is not defined"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "result = np.zeros((N, vae.image_dim))\n",
    "for i in range(N):\n",
    "    trace = posterior()\n",
    "    data = dict()\n",
    "    noise_vars = ['Ny_' + s for s in vae.label_names] + ['Nx', 'Nz']\n",
    "    for name in noise_vars:\n",
    "        data[name] = trace.nodes[name]['value']\n",
    "        \n",
    "    con_obj = pyro.condition(intervened_model, data = data)\n",
    "    result[i] = con_obj(vae, x)[0]\n",
    "    \n",
    "# now generate density image\n",
    "density_comp(x, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_map(img):\n",
    "    dens_img = np.mean(img, axis=0).reshape(64,64)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(dens_img, interpolation='nearest', cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "def density_comp(img, recons):\n",
    "    dens_img = np.mean(recons, axis=0).reshape(64,64)\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(121)\n",
    "    plt.imshow(img.cpu().reshape(64,64), interpolation='nearest', cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig.add_subplot(122)\n",
    "    plt.imshow(dens_img, interpolation='nearest', cmap=\"Greys_r\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
