{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.z_dim = z_dim\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc31 = nn.Linear(1000, z_dim)  # mu values\n",
    "        self.fc32 = nn.Linear(1000, z_dim)  # sigma values\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        # define the forward computation on the image xs and label ys\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        xs = xs.reshape(-1, self.image_dim)\n",
    "        #now concatenate the image and label\n",
    "        inputs = torch.cat((xs,ys), -1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc31(hidden2)\n",
    "        z_scale = torch.exp(self.fc32(hidden2))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        hidden_dim = 1000\n",
    "        self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, image_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, zs, ys):\n",
    "        # define the forward computation on the latent z and label y\n",
    "        # first concatenate z and y\n",
    "        inputs = torch.cat((zs, ys),-1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        hidden3 = self.softplus(self.fc3(hidden2))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc4(hidden3))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.image_dim = 64**2\n",
    "        self.label_shape = np.array((1,3,6,40,32,32))\n",
    "        self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))\n",
    "        self.label_dim = np.sum(self.label_shape)\n",
    "        self.z_dim = 50                                           \n",
    "        self.allow_broadcast = config_enum == 'parallel'\n",
    "        self.use_cuda = use_cuda\n",
    "        self.aux_loss_multiplier = aux_loss_multiplier\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the paramters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # dsprites label (latent)\n",
    "        p(y|x) = categorical(I/10.)     # which digit (supervised)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"cvae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "    \n",
    "            loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label \n",
    "        loc, scale are given by a neural network `encoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample (and score) the latent handwriting-style with the variational\n",
    "            # distribution q(z|x) = normal(loc(x),scale(x))\n",
    "    \n",
    "            loc, scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "            \n",
    "    def remap_y(self, ys):\n",
    "        new_ys = []\n",
    "        options = dict(dtype=ys.dtype, device=ys.device)\n",
    "        for i, label_length in enumerate(self.label_shape):\n",
    "            prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)\n",
    "            new_ys.append(pyro.sample(\"y_%s\" % self.label_names[i], dist.OneHotCategorical(prior), \n",
    "                                   obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))\n",
    "        new_ys = torch.cat(new_ys, -1)\n",
    "        return new_ys.to(torch.float32)\n",
    "            \n",
    "    def reconstruct_image(self, xs, ys):\n",
    "        # backward\n",
    "        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
    "        # forward\n",
    "        loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "        return dist.Bernoulli(loc).to_event(1).sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):\n",
    "    train_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(train_y.astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    test_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(test_y.astype(np.float32))\n",
    "    )    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\"train\":train_loader, \"test\":test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip = np.load(\n",
    "    'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
    "    encoding = 'bytes',\n",
    "    allow_pickle=True\n",
    ")\n",
    "imgs = dataset_zip['imgs']\n",
    "labels = dataset_zip['latents_classes']\n",
    "label_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
    "label_names = dataset_zip['metadata'][()][b'latents_names']\n",
    "\n",
    "# Sample imgs randomly\n",
    "indices_sampled = np.arange(imgs.shape[0])\n",
    "np.random.shuffle(indices_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "labels_sampled = labels[indices_sampled]\n",
    "\n",
    "data_loaders = setup_data_loaders(\n",
    "    imgs_sampled[1000:],\n",
    "    imgs_sampled[:1000],\n",
    "    labels_sampled[1000:],\n",
    "    labels_sampled[:1000],\n",
    "    batch_size=256,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for xs,ys in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(xs, ys)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for xs, ys in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(xs, ys)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 100\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### FOR SAVING AND LOADING MODEL\n",
    "################################\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "PATH = \"trained_model.save\"\n",
    "\n",
    "# new model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# save current model\n",
    "# torch.save(vae.state_dict(), PATH)\n",
    "\n",
    "# to load params from trained model\n",
    "vae = CVAE(use_cuda=USE_CUDA)\n",
    "vae.load_state_dict(torch.load(PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a12b6c267b34b24a0341f61a252e3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 15.2831\n",
      "[epoch 000] average test loss: 17.6650\n",
      "[epoch 001]  average training loss: 14.5343\n",
      "[epoch 002]  average training loss: 13.9415\n",
      "[epoch 003]  average training loss: 13.4132\n",
      "[epoch 004]  average training loss: 12.9575\n",
      "[epoch 005]  average training loss: 12.5879\n",
      "[epoch 005] average test loss: 13.9519\n",
      "[epoch 006]  average training loss: 12.2192\n",
      "[epoch 007]  average training loss: 11.8638\n",
      "[epoch 008]  average training loss: 11.5776\n",
      "[epoch 009]  average training loss: 11.2881\n",
      "[epoch 010]  average training loss: 11.0081\n",
      "[epoch 010] average test loss: 12.4815\n",
      "[epoch 011]  average training loss: 11.1047\n",
      "[epoch 012]  average training loss: 10.5209\n",
      "[epoch 013]  average training loss: 10.2897\n",
      "[epoch 014]  average training loss: 10.0783\n",
      "[epoch 015]  average training loss: 9.8712\n",
      "[epoch 015] average test loss: 11.8235\n",
      "[epoch 016]  average training loss: 9.6722\n",
      "[epoch 017]  average training loss: 9.4888\n",
      "[epoch 018]  average training loss: 9.3033\n",
      "[epoch 019]  average training loss: 9.1291\n",
      "[epoch 020]  average training loss: 8.9643\n",
      "[epoch 020] average test loss: 10.8259\n",
      "[epoch 021]  average training loss: 8.8299\n",
      "[epoch 022]  average training loss: 8.6701\n",
      "[epoch 023]  average training loss: 8.5306\n",
      "[epoch 024]  average training loss: 8.3834\n",
      "[epoch 025]  average training loss: 8.2576\n",
      "[epoch 025] average test loss: 10.3652\n",
      "[epoch 026]  average training loss: 23189.7976\n",
      "[epoch 027]  average training loss: 239.6831\n",
      "[epoch 028]  average training loss: 7.9007\n",
      "[epoch 029]  average training loss: 7.7838\n",
      "[epoch 030]  average training loss: 7.6848\n",
      "[epoch 030] average test loss: 9.9376\n",
      "[epoch 031]  average training loss: 7.5722\n",
      "[epoch 032]  average training loss: 7.4608\n",
      "[epoch 033]  average training loss: 7.3689\n",
      "[epoch 034]  average training loss: 7.2695\n",
      "[epoch 035]  average training loss: 7.1809\n",
      "[epoch 035] average test loss: 9.7329\n",
      "[epoch 036]  average training loss: 7.1184\n",
      "[epoch 037]  average training loss: 6.9998\n",
      "[epoch 038]  average training loss: 9.3081\n",
      "[epoch 039]  average training loss: 6.8295\n",
      "[epoch 040]  average training loss: 7.5134\n",
      "[epoch 040] average test loss: 9.3493\n",
      "[epoch 041]  average training loss: 6.6828\n",
      "[epoch 042]  average training loss: 6.6096\n",
      "[epoch 043]  average training loss: 6.5347\n",
      "[epoch 044]  average training loss: 6.4644\n",
      "[epoch 045]  average training loss: 6.3954\n",
      "[epoch 045] average test loss: 9.2391\n",
      "[epoch 046]  average training loss: 6.3359\n",
      "[epoch 047]  average training loss: 9.6716\n",
      "[epoch 048]  average training loss: 6.2103\n",
      "[epoch 049]  average training loss: 6.1419\n",
      "[epoch 050]  average training loss: 6.0903\n",
      "[epoch 050] average test loss: 9.0349\n",
      "[epoch 051]  average training loss: 6.0305\n",
      "[epoch 052]  average training loss: 5.9678\n",
      "[epoch 053]  average training loss: 5.9208\n",
      "[epoch 054]  average training loss: 5.8682\n",
      "[epoch 055]  average training loss: 5.8106\n",
      "[epoch 055] average test loss: 8.8320\n",
      "[epoch 056]  average training loss: 5.7644\n",
      "[epoch 057]  average training loss: 5.7643\n",
      "[epoch 058]  average training loss: 5.6610\n",
      "[epoch 059]  average training loss: 5.6181\n",
      "[epoch 060]  average training loss: 5.5760\n",
      "[epoch 060] average test loss: 8.9104\n",
      "[epoch 061]  average training loss: 5.5301\n",
      "[epoch 062]  average training loss: 5.4844\n",
      "[epoch 063]  average training loss: 5.4342\n",
      "[epoch 064]  average training loss: 5.3990\n",
      "[epoch 065]  average training loss: 5.3497\n",
      "[epoch 065] average test loss: 8.8160\n",
      "[epoch 066]  average training loss: 5.3120\n",
      "[epoch 067]  average training loss: 5.2713\n",
      "[epoch 068]  average training loss: 5.2337\n",
      "[epoch 069]  average training loss: 5.1955\n",
      "[epoch 070]  average training loss: 5.1590\n",
      "[epoch 070] average test loss: 8.9201\n",
      "[epoch 071]  average training loss: 5.1143\n",
      "[epoch 072]  average training loss: 5.0823\n",
      "[epoch 073]  average training loss: 5.0486\n",
      "[epoch 074]  average training loss: 5.0096\n",
      "[epoch 075]  average training loss: 4.9785\n",
      "[epoch 075] average test loss: 8.8781\n",
      "[epoch 076]  average training loss: 4.9415\n",
      "[epoch 077]  average training loss: 4.9120\n",
      "[epoch 078]  average training loss: 4.8807\n",
      "[epoch 079]  average training loss: 4.8462\n",
      "[epoch 080]  average training loss: 4.8169\n",
      "[epoch 080] average test loss: 8.9105\n",
      "[epoch 081]  average training loss: 4.7809\n",
      "[epoch 082]  average training loss: 4.7538\n",
      "[epoch 083]  average training loss: 4.7247\n",
      "[epoch 084]  average training loss: 4.6927\n",
      "[epoch 085]  average training loss: 4.6726\n",
      "[epoch 085] average test loss: 8.8387\n",
      "[epoch 086]  average training loss: 4.6403\n",
      "[epoch 087]  average training loss: 4.6031\n",
      "[epoch 088]  average training loss: 4.5867\n",
      "[epoch 089]  average training loss: 4.5555\n",
      "[epoch 090]  average training loss: 4.5242\n",
      "[epoch 090] average test loss: 8.8349\n",
      "[epoch 091]  average training loss: 4.5004\n",
      "[epoch 092]  average training loss: 4.4727\n",
      "[epoch 093]  average training loss: 4.4483\n",
      "[epoch 094]  average training loss: 4.4288\n",
      "[epoch 095]  average training loss: 4.3992\n",
      "[epoch 095] average test loss: 8.9367\n",
      "[epoch 096]  average training loss: 4.3691\n",
      "[epoch 097]  average training loss: 4.3501\n",
      "[epoch 098]  average training loss: 4.3234\n",
      "[epoch 099]  average training loss: 4.3052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "\n",
    "VERBOSE = True\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in pbar:\n",
    "    total_epoch_loss_train = train(svi, data_loaders[\"train\"], use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    if VERBOSE:\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, data_loaders[\"test\"], use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        if VERBOSE:\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the reconstruction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loaders[\"train\"])\n",
    "xs, ys = next(data_iter)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cuda()\n",
    "    ys = ys.cuda()\n",
    "rs = vae.reconstruct_image(xs, ys)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cpu()\n",
    "    rs = rs.cpu()\n",
    "originals = xs.numpy().reshape(-1, 64,64)\n",
    "recons = rs.numpy().reshape(-1,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b053b1db954f6291013b96b1e416b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', max=256), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def f(x):\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(originals[x], cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(recons[x], cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(f, x=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCM(vae, mu, sigma):\n",
    "    z_dim = vae.z_dim\n",
    "    Nx = pyro.sample(\"Nx\", dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)))\n",
    "    Nz = pyro.sample(\"Nz\", dist.Normal(torch.zeros(z_dim), torch.ones(z_dim)))\n",
    "    Ny = []\n",
    "    Y = []\n",
    "    ys = []\n",
    "    m = torch.distributions.gumbel.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n",
    "    for label_id in range(6):\n",
    "        name = vae.label_names[label_id]\n",
    "        length = vae.label_shape[label_id]\n",
    "        new = pyro.sample(\"Ny_%s\"%name, dist.Uniform(torch.zeros(length), torch.ones(length)) )\n",
    "        Ny.append(new)\n",
    "        gumbel_vars = torch.tensor([m.sample() for _ in range(length)])\n",
    "        max_ind = torch.argmax(torch.log(new) + gumbel_vars).item()\n",
    "        Y.append(pyro.sample(\"Y_%s\"%name, dist.Delta(torch.tensor(max_ind))))\n",
    "        ys.append(torch.nn.functional.one_hot(torch.tensor(max_ind), int(length)))\n",
    "                 \n",
    "    Y = torch.tensor(Y)\n",
    "    ys = torch.cat(ys).to(torch.float32).reshape(1,-1).cuda()\n",
    "    Z = pyro.sample(\"Z\", dist.Delta(mu + Nz*sigma))\n",
    "    zs = Z.cuda()\n",
    "    p = vae.decoder.forward(zs,ys).cpu()\n",
    "    X = pyro.sample(\"X\", dist.Delta(Nx < p))\n",
    "    return X, Y, Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = next(data_iter)\n",
    "x = xs[0].reshape(1,-1).cuda()\n",
    "y = ys[0].reshape(1,-1).cuda()\n",
    "mu, sigma = vae.encoder.forward(x,vae.remap_y(y))\n",
    "mu = mu.cpu()\n",
    "sigma = sigma.cpu()\n",
    "SCM(vae, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
