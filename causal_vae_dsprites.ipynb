{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions.constraints as constraints\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.z_dim = z_dim\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc31 = nn.Linear(1000, z_dim)  # mu values\n",
    "        self.fc32 = nn.Linear(1000, z_dim)  # sigma values\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        # define the forward computation on the image xs and label ys\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        xs = xs.reshape(-1, self.image_dim)\n",
    "        #now concatenate the image and label\n",
    "        inputs = torch.cat((xs,ys), -1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc31(hidden2)\n",
    "        z_scale = torch.exp(self.fc32(hidden2))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim, label_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        hidden_dim = 1000\n",
    "        self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, image_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, zs, ys):\n",
    "        # define the forward computation on the latent z and label y\n",
    "        # first concatenate z and y\n",
    "        inputs = torch.cat((zs, ys),-1)\n",
    "        # then compute the hidden units\n",
    "        hidden1 = self.softplus(self.fc1(inputs))\n",
    "        hidden2 = self.softplus(self.fc2(hidden1))\n",
    "        hidden3 = self.softplus(self.fc3(hidden2))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc4(hidden3))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.image_dim = 64**2\n",
    "        self.label_shape = np.array((1,3,6,40,32,32))\n",
    "        self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))\n",
    "        self.label_dim = np.sum(self.label_shape)\n",
    "        self.z_dim = 50                                    \n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the paramters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)\n",
    "\n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # dsprites label (latent)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"cvae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "    \n",
    "            loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, ys):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label \n",
    "        loc, scale are given by a neural network `encoder`\n",
    "\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample (and score) the latent handwriting-style with the variational\n",
    "            # distribution q(z|x) = normal(loc(x),scale(x))\n",
    "    \n",
    "            loc, scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "            \n",
    "    def remap_y(self, ys):\n",
    "        new_ys = []\n",
    "        options = dict(dtype=ys.dtype, device=ys.device)\n",
    "        for i, label_length in enumerate(self.label_shape):\n",
    "            prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)\n",
    "            new_ys.append(pyro.sample(\"y_%s\" % self.label_names[i], dist.OneHotCategorical(prior), \n",
    "                                   obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))\n",
    "        new_ys = torch.cat(new_ys, -1)\n",
    "        return new_ys.to(torch.float32)\n",
    "            \n",
    "    def reconstruct_image(self, xs, ys):\n",
    "        # backward\n",
    "        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))\n",
    "        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()\n",
    "        # forward\n",
    "        loc = self.decoder.forward(zs, self.remap_y(ys))\n",
    "        return dist.Bernoulli(loc).to_event(1).sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):\n",
    "    train_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(train_y.astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    test_dset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),\n",
    "        torch.from_numpy(test_y.astype(np.float32))\n",
    "    )    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\"train\":train_loader, \"test\":test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip = np.load(\n",
    "    'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',\n",
    "    encoding = 'bytes',\n",
    "    allow_pickle=True\n",
    ")\n",
    "imgs = dataset_zip['imgs']\n",
    "labels = dataset_zip['latents_classes']\n",
    "label_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
    "label_names = dataset_zip['metadata'][()][b'latents_names']\n",
    "\n",
    "# Sample imgs randomly\n",
    "indices_sampled = np.arange(imgs.shape[0])\n",
    "np.random.shuffle(indices_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "labels_sampled = labels[indices_sampled]\n",
    "\n",
    "data_loaders = setup_data_loaders(\n",
    "    imgs_sampled[1000:],\n",
    "    imgs_sampled[:1000],\n",
    "    labels_sampled[1000:],\n",
    "    labels_sampled[:1000],\n",
    "    batch_size=256,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ae45f1988e46e19ccc4b27f64dc946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='shape', max=2), IntSlider(value=4, description='scale', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.find_in_dataset(shape, scale, orient, posX, posY)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_names = ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "y_shapes = np.array((3,6,40,32,32))\n",
    "img_dict = {}\n",
    "for i, img in enumerate(imgs_sampled):\n",
    "    img_dict[tuple(labels_sampled[i])] = img\n",
    "    \n",
    "def find_in_dataset(shape, scale, orient, posX, posY):\n",
    "    fig = plt.figure()\n",
    "    img = img_dict[(0, shape, scale, orient, posX, posY)]\n",
    "    plt.imshow(img.reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(find_in_dataset, shape=widgets.IntSlider(min=0, max=2, step=1, value=npr.randint(2)),\n",
    "                          scale=widgets.IntSlider(min=0, max=5, step=1, value=npr.randint(5)),\n",
    "                            orient=widgets.IntSlider(min=0, max=39, step=1, value=npr.randint(39)),\n",
    "                            posX=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)),\n",
    "                            posY=widgets.IntSlider(min=0, max=31, step=1, value=npr.randint(31)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_specific_data(args=dict(), cuda=False):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        cuda - bool to indicate whether the output should be placed on GPU\n",
    "    '''\n",
    "    names_dict = {'shape': 1, 'scale': 2, 'orientation': 3, 'posX': 4, 'posY': 5}\n",
    "    selected_ind = np.ones(imgs.shape[0], dtype=bool)\n",
    "    for k,v in args.items():\n",
    "        col_id = names_dict[k]\n",
    "        selected_ind = np.bitwise_and(selected_ind, labels[:, col_id] == v)\n",
    "    ind = np.random.choice(np.arange(imgs.shape[0])[selected_ind])\n",
    "    x = torch.from_numpy(imgs[ind].reshape(1,64**2).astype(np.float32))\n",
    "    y = torch.from_numpy(labels[ind].reshape(1,6).astype(np.float32))\n",
    "    if not cuda:\n",
    "        return x,y\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    return x,y\n",
    "\n",
    "def plot_image(x):\n",
    "    x = x.cpu()\n",
    "    plt.figure()\n",
    "    plt.imshow(x.reshape(64,64), interpolation='nearest', cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "def see_specific_image(args=dict(), verbose=True):\n",
    "    '''\n",
    "    use this function to get examples of data with specific class labels\n",
    "    inputs: \n",
    "        args - dictionary whose keys can include {shape, scale, orientation,\n",
    "                posX, posY} and values can include any integers less than the \n",
    "                corresponding size of that label dimension\n",
    "        verbose - bool to indicate whether the full class label should be written \n",
    "                    as the title of the plot\n",
    "    '''\n",
    "    x,y = get_specific_data(args, cuda=False)\n",
    "    plot_image(x)\n",
    "    if verbose:\n",
    "        string = ''\n",
    "        for i, s in enumerate(['Shape', 'Scale', 'Orientation', 'PosX', 'PosY']):\n",
    "            string += '%s: %d, ' % (s, int(y[0][i+1]))\n",
    "            if i == 2:\n",
    "                string = string[:-2] + '\\n'\n",
    "        plt.title(string[:-2])\n",
    "        \n",
    "def compare_reconstruction(original, recon):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array for recon image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(recon.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstruction')\n",
    "    \n",
    "def compare_to_density(original, recons):\n",
    "    \"\"\"\n",
    "    compare two images side by side\n",
    "    inputs:\n",
    "        original - array for original image\n",
    "        recon - array of multiple recon images\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(original.cpu().reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(torch.mean(recons.cpu(), 0).reshape(64,64), cmap='Greys_r',  interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstructions')\n",
    "\n",
    "        \n",
    "see_specific_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training or Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for xs,ys in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(xs, ys)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for xs, ys in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(xs, ys)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 0\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### FOR SAVING AND LOADING MODEL\n",
    "################################\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "PATH = \"trained_model.save\"\n",
    "\n",
    "# new model\n",
    "# vae = CVAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# save current model\n",
    "# torch.save(vae.state_dict(), PATH)\n",
    "\n",
    "# to load params from trained model\n",
    "vae = CVAE(use_cuda=USE_CUDA)\n",
    "vae.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935fefbb79a14937bf871bd1a5a0bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "\n",
    "VERBOSE = True\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in pbar:\n",
    "    total_epoch_loss_train = train(svi, data_loaders[\"train\"], use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    if VERBOSE:\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, data_loaders[\"test\"], use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        if VERBOSE:\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reconstruction accuracy of trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c71e47ca164895b547e33f005f85e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='data id', max=256), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(data_loaders[\"train\"])\n",
    "xs, ys = next(data_iter)\n",
    "if USE_CUDA:\n",
    "    xs = xs.cuda()\n",
    "    ys = ys.cuda()\n",
    "rs = vae.reconstruct_image(xs, ys)\n",
    "\n",
    "def f(i):\n",
    "    compare_reconstruction(xs[i], rs[i])\n",
    "    \n",
    "interact(f, i=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0,description='data id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Structural Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4,  10,  50,  82, 114])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dims = vae.label_shape\n",
    "label_dim_offsets = np.cumsum(label_dims)\n",
    "label_dim_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCM():\n",
    "    def __init__(self, vae, mu, sigma):\n",
    "        self.vae = vae\n",
    "        self.image_dim = vae.image_dim\n",
    "        self.z_dim = vae.z_dim\n",
    "        \n",
    "        mu = mu.cpu()\n",
    "        sigma = sigma.cpu()\n",
    "        \n",
    "        # these are used for f_X\n",
    "        self.label_dims = vae.label_shape\n",
    "        \n",
    "        def f_X(Y, Z, N):\n",
    "            zs = Z.cuda()\n",
    "            \n",
    "            # convert the labels to one hot\n",
    "            ys = [torch.tensor([0])]\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[0]), int(self.label_dims[1])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[1]), int(self.label_dims[2])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[2]), int(self.label_dims[3])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[3]), int(self.label_dims[4])))\n",
    "            ys.append(torch.nn.functional.one_hot(torch.tensor(Y[4]), int(self.label_dims[5])))\n",
    "            ys = torch.cat(ys).to(torch.float32).reshape(1,-1).cuda()\n",
    "            \n",
    "            p = vae.decoder.forward(zs, ys)\n",
    "            return (N < p.cpu()).type(torch.float)\n",
    "        \n",
    "        def f_Y(N):\n",
    "            m = torch.distributions.gumbel.Gumbel(torch.zeros(N.size(0)), torch.ones(N.size(0)))\n",
    "            return torch.argmax(torch.add(torch.log(N), m.sample())).item()\n",
    "        \n",
    "        def f_Z(N):\n",
    "            return N * sigma + mu\n",
    "        \n",
    "        def model(noise):\n",
    "            N_X = pyro.sample( 'N_X', noise['N_X'] )\n",
    "            # There are 5 Y variables and they will be\n",
    "            # denoted using the index in the sequence \n",
    "            # that they are stored in as vae.label_names:\n",
    "            # ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "            N_Y_1 = pyro.sample( 'N_Y_1', noise['N_Y_1'] )\n",
    "            N_Y_2 = pyro.sample( 'N_Y_2', noise['N_Y_2'] )\n",
    "            N_Y_3 = pyro.sample( 'N_Y_3', noise['N_Y_3'] )\n",
    "            N_Y_4 = pyro.sample( 'N_Y_4', noise['N_Y_4'] )\n",
    "            N_Y_5 = pyro.sample( 'N_Y_5', noise['N_Y_5'] ) \n",
    "            N_Z = pyro.sample( 'N_Z', noise['N_Z'] )\n",
    "             \n",
    "            Z = pyro.sample('Z', dist.Normal( f_Z( N_Z ), 1e-1) )\n",
    "            Y_1_mu = f_Y(N_Y_1)\n",
    "            Y_2_mu = f_Y(N_Y_2)\n",
    "            Y_3_mu = f_Y(N_Y_3)\n",
    "            Y_4_mu = f_Y(N_Y_4)\n",
    "            Y_5_mu = f_Y(N_Y_5)\n",
    "            Y_1 = pyro.sample('Y_1', dist.Normal( Y_1_mu, 1e-1) )\n",
    "            Y_2 = pyro.sample('Y_2', dist.Normal( Y_2_mu, 1e-1) )\n",
    "            Y_3 = pyro.sample('Y_3', dist.Normal( Y_3_mu, 1e-1) )\n",
    "            Y_4 = pyro.sample('Y_4', dist.Normal( Y_4_mu, 1e-1) )\n",
    "            Y_5 = pyro.sample('Y_5', dist.Normal( Y_5_mu, 1e-1) )\n",
    "            Y_mu = (Y_1_mu, Y_2_mu, Y_3_mu, Y_4_mu, Y_5_mu)\n",
    "            X = pyro.sample('X', dist.Normal( f_X( Y_mu, Z, N_X ), 1e-1) )\n",
    "            \n",
    "            noise_samples = N_X, (N_Y_1, N_Y_2, N_Y_3, N_Y_4, N_Y_5), N_Z\n",
    "            variable_samples = X, (Y_1, Y_2, Y_3, Y_4, Y_5), Z\n",
    "            \n",
    "            return variable_samples, noise_samples\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.init_noise = {\n",
    "            'N_X'   : dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)),\n",
    "            'N_Z'   : dist.Normal(torch.zeros(vae.z_dim), torch.ones(vae.z_dim)),\n",
    "            'N_Y_1' : dist.Uniform(torch.zeros(label_dims[1]),torch.ones(self.label_dims[1])),\n",
    "            'N_Y_2' : dist.Uniform(torch.zeros(label_dims[2]),torch.ones(self.label_dims[2])),\n",
    "            'N_Y_3' : dist.Uniform(torch.zeros(label_dims[3]),torch.ones(self.label_dims[3])),\n",
    "            'N_Y_4' : dist.Uniform(torch.zeros(label_dims[4]),torch.ones(self.label_dims[4])),\n",
    "            'N_Y_5' : dist.Uniform(torch.zeros(label_dims[5]),torch.ones(self.label_dims[5]))            \n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_noise_svi(self, obs_data):\n",
    "        # assume all noise variables are normal distributions\n",
    "        # use svi to find out the mu, sigma of the distributions\n",
    "        # for the condition outlined in obs_data\n",
    "        def guide(noise):\n",
    "            # create params with constraints\n",
    "            mu = {'N_X': pyro.param('N_X_mu', 0.5*torch.ones(self.image_dim),\n",
    "                                    constraint = constraints.interval(0., 1.)),\n",
    "                  'N_Z': pyro.param('N_Z_mu', torch.zeros(self.z_dim),\n",
    "                                    constraint = constraints.interval(-3., 3.)),\n",
    "                  'N_Y_1': pyro.param('N_Y_1_mu', 0.5*torch.ones(self.label_dims[1]),\n",
    "                                    constraint = constraints.interval(0., 1.)),\n",
    "                  'N_Y_2': pyro.param('N_Y_2_mu', 0.5*torch.ones(self.label_dims[2]),\n",
    "                                    constraint = constraints.interval(0., 1.)),\n",
    "                  'N_Y_3': pyro.param('N_Y_3_mu', 0.5*torch.ones(self.label_dims[3]),\n",
    "                                    constraint = constraints.interval(0., 1.)),\n",
    "                  'N_Y_4': pyro.param('N_Y_4_mu', 0.5*torch.ones(self.label_dims[4]),\n",
    "                                    constraint = constraints.interval(0., 1.)),\n",
    "                  'N_Y_5': pyro.param('N_Y_5_mu', 0.5*torch.ones(self.label_dims[5]),\n",
    "                                    constraint = constraints.interval(0., 1.))\n",
    "                }\n",
    "            sigma = {'N_X': pyro.param('N_X_sigma', 0.1*torch.ones(self.image_dim),\n",
    "                                    constraint = constraints.interval(0.0001, 0.5)),\n",
    "                      'N_Z': pyro.param('N_Z_sigma', torch.ones(self.z_dim),\n",
    "                                        constraint = constraints.interval(0.0001, 3.)),\n",
    "                      'N_Y_1': pyro.param('N_Y_1_sigma', 0.1*torch.ones(self.label_dims[1]),\n",
    "                                        constraint = constraints.interval(0.0001, 0.5)),\n",
    "                      'N_Y_2': pyro.param('N_Y_2_sigma', 0.1*torch.ones(self.label_dims[2]),\n",
    "                                        constraint = constraints.interval(0.0001, 0.5)),\n",
    "                      'N_Y_3': pyro.param('N_Y_3_sigma', 0.1*torch.ones(self.label_dims[3]),\n",
    "                                        constraint = constraints.interval(0.0001, 0.5)),\n",
    "                      'N_Y_4': pyro.param('N_Y_4_sigma', 0.1*torch.ones(self.label_dims[4]),\n",
    "                                        constraint = constraints.interval(0.0001, 0.5)),\n",
    "                      'N_Y_5': pyro.param('N_Y_5_sigma', 0.1*torch.ones(self.label_dims[5]),\n",
    "                                    constraint = constraints.interval(0.0001, 0.5))\n",
    "                }\n",
    "                  \n",
    "            for noise_term in noise.keys():\n",
    "                pyro.sample(noise_term, dist.Normal(mu[noise_term], sigma[noise_term]))\n",
    "        \n",
    "        obs_model = pyro.condition(self.model, obs_data)\n",
    "        pyro.clear_param_store()\n",
    "        svi = SVI(\n",
    "            model= obs_model,\n",
    "            guide= guide,\n",
    "            optim= Adam({\"lr\": 1e-3}),\n",
    "            loss=Trace_ELBO()\n",
    "        )\n",
    "        \n",
    "        num_steps = 1000\n",
    "        samples = defaultdict(list)\n",
    "        for t in range(num_steps):\n",
    "            svi.step(self.init_noise)\n",
    "            \n",
    "        # now determine new noise variables\n",
    "        for noise in initial_noise.keys():\n",
    "            mu = '{}_mu'.format(noise)\n",
    "            sigma = '{}_sigma'.format(noise)\n",
    "            samples[mu].append(pyro.param(mu).item())\n",
    "            samples[sigma].append(pyro.param(sigma).item())\n",
    "        means = {k: torch.mean(torch.cat(v, 0),0) for k, v in samples.items()}\n",
    "        \n",
    "        updated_noise = {\n",
    "            'N_X': dist.Normal(means['N_X_mu'], means['N_X_sigma']),\n",
    "            'N_Z': dist.Normal(means['N_Z_mu'], means['N_Z_sigma']),\n",
    "            'N_Y_1': dist.Normal(means['N_Y_1_mu'], means['N_Y_1_sigma']),\n",
    "            'N_Y_2': dist.Normal(means['N_Y_2_mu'], means['N_Y_2_sigma']),\n",
    "            'N_Y_3': dist.Normal(means['N_Y_3_mu'], means['N_Y_3_sigma']),\n",
    "            'N_Y_4': dist.Normal(means['N_Y_4_mu'], means['N_Y_4_sigma']),\n",
    "            'N_Y_5': dist.Normal(means['N_Y_5_mu'], means['N_Y_5_sigma']),\n",
    "        }\n",
    "        \n",
    "        return updated_noise\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.model(self.init_noise)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  2.,  1.,  5., 19., 28.]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAENCAYAAAAVEjAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA6tJREFUeJzt3cFqwkAUQNFO8f9/+XVbSpHcGBOC5yxFZVaXN6ND1sx8AWz1ffUCgHsRDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSB5XL2APdZabtnBm83M+u91kwaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQiAaQPK5eAMzMpvettd68ErYwaQCJaACJaACJMw1Ot/UM49nnnG9cx6QBJKIBJLYn3NLfLY7tynlMGkAiGkAiGkAiGkAiGkAiGkDiJ1cOs/efntyLSQNIRANIbE94iS3J5zFpAIloAIloAIloAIloAIloAIloAIloAIloAIloAIloAIloAIloAIlbrrzk9/NG3Hj9DCYNIBENILE94TBbH434bBuz9zs8Uf48Jg0gEQ0gEQ0gcabB6Y44c3BucR2TBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCIBpCsmbl6DcCNmDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSARDSA5AdkAioizk/1bwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing scm functions\n",
    "x, y = get_specific_data(cuda=True)\n",
    "mu, sigma = vae.encoder.forward(x,vae.remap_y(y))\n",
    "scm = SCM(vae, mu.cpu(), sigma.cpu())\n",
    "print(y)\n",
    "plot_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at site \"N_X\", invalid log_prob shape\n  Expected [], actual [4096]\n  Try one of the following fixes:\n  - enclose the batched tensor in a with plate(...): context\n  - .to_event(...) the distribution being sampled\n  - .permute() data dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-e22c3f786618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcond_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_noise_svi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-276-8176258ce037>\u001b[0m in \u001b[0;36mupdate_noise_svi\u001b[0;34m(self, obs_data)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# now determine new noise variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 52\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mcheck_site_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_plate_nesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cs7180/lib/python3.6/site-packages/pyro/util.py\u001b[0m in \u001b[0;36mcheck_site_shape\u001b[0;34m(site, max_plate_nesting)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;34m'- enclose the batched tensor in a with plate(...): context'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;34m'- .to_event(...) the distribution being sampled'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 '- .permute() data dimensions']))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Check parallel dimensions on the left of max_plate_nesting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at site \"N_X\", invalid log_prob shape\n  Expected [], actual [4096]\n  Try one of the following fixes:\n  - enclose the batched tensor in a with plate(...): context\n  - .to_event(...) the distribution being sampled\n  - .permute() data dimensions"
     ]
    }
   ],
   "source": [
    "cond_data = {}\n",
    "for i in range(1, 6):\n",
    "    cond_data[\"Y_{}\".format(i)] = torch.tensor(y[0,i].cpu()).to(torch.float32)\n",
    "\n",
    "\n",
    "cond_noise = scm.update_noise_svi(cond_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check that do-operation should work on trained network\n",
    "\n",
    "At one point I was concerned that the network was encoding too much information in the latent encoding z and thus conditioning on the labels wasnt enough to change the output.  According to the results below, that is not the case.  We should be able to condition on the labels and get sensible outputs from the conditioned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  2.,  2., 15., 25., 13.]], device='cuda:0')\n",
      "tensor([[ 0.,  0.,  2., 15., 25., 13.]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAADcCAYAAACBHI1wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAD9ZJREFUeJzt3X+wXGV9x/HPJwn5IYi0AioVTGiEARyqwakzUgMjwoBKp51qp512SoAyI9NfODpTq9M21mmxTFrbsSKNIY0zlWqhaB2sKYJJk1DtSKq1GIOQkBIafkSsQDTh5sfTP55nb05Ozu537969d3fvfb9mdjb3nOec8+zZZ/dznnPOs3FKSQAAdDJn0BUAAAw/wgIAECIsAAAhwgIAECIsAAAhwgIAECIshpztjbaT7Uv7tL5dZX2L+7G+Ua0DMF1sryvtfcWg6zIZhAWAoWJ7RflyXTfoukRsLy513TXouky1eYOuAEK/Ieklkh7v0/ouk3SCpP/t0/oAdPYHkj4q6clBV2QyCIshl1LqV0i01rejn+sD0FlK6UmNeFBInIbqO9tLbK8u5+VftP2s7X+1/c6GsuPXI2y/zfa9tn9Qpr2+XqZh+VfZXmP7SdsHbH/X9u/bntvuukA3022/3fZm2y/Yft72etvL2rzeXyrnZLfZfs72ftvbba+yfWrPO3IGK/s5ObvR9lbb+2z/sFbuJNsftP2f5b34se1v2X6/7fkd1v8O21+0/ZTtMdt7bG+w/bsNZefbvsn2g5VtfNv2H9o+qaH8+Cki2y+z/de2d5e2vsP2H9s+7iDU9iLbv2P7G7b3lva6x/Ym2x+slNso6e/Kn9dU9tUxp6Vq7fWXbW8p7S/ZPqW6n9vso46nj2yfbvvPyr7YV/bNdtu32X5dKbNS0mNlkdfU6rqrsq621yyGbf93lFLi0aeHpDdLek5SkvQ9Sf8gaYOkQ2XazbXyG8v02yQdkfRNSXdI2izpwlqZS2vLvlr51FRSPqX0OUlflrRf0l2SdpV5i2vLRdNvlnS41OEfJT1apu+TdE7Daz4k6XlJXy/l/0XS02WZxySd1rBMYx1my6O89iTpVkkHJX21tJUHKmXOlLS9lHtS0pck3SPp+2XaBknza+u1pE+V+Yclfa20p/skPZU/7seUXyRpUyn/vKR/Lm2ntY1vSzq1tsyKMu8LkraV9d4p6SuSDpR5q2vLzCn1TZL+r7yOO8q0pyUdqJT9gKQtpeyjktZVHr/Z0IZuLc//Xtb5oKSXVfdzm/dgcZm/q2HeRZU2/HR5rXdK2lr268pS7hfK/mp9Pqp1XVVZ37pSZsWw7/+O7XbQH5yZ8pC0UNLu8ob8qSRX5r1Z0gtl3lWV6Rt19ItjRZv1tspcWpv+xTL9LkkLK9PPkbSnst7FteV2BdP3S7qkMv0ESZ8v89Y21O/dkhY17Is1ZZnbGpZprMNseVTemx9IekPDfCuHb5K0StKCyrxTJK0v8/6kttz7yvTH6+uVNFfS1bVpq0r5b0k6vTL9ZOUAS5I+V1um9WWVJN1da3tvUj54OFJ9byVdUso/KOnEhnq9tc021nXYh602NCbpik77uc28xWoIC0kvVT74SpL+QscH8pmSLorWU1tmnZrDYij3f9vXMegPzkx5KF+ITspHg3Ma5q8s8++rTNtYpq3vsN5WmUtrDfSI8pHEKxuWubHSoBbX5u0Kpn+0YX1vLPMem8D+WKR81Ly3YV5jHWbLo/LefKDN/LeX+RtVOeiozH+VpBeVj0Bdpp2go0eky7t8f/aV8hc3zF9avngOSzqrMr31ZfW8mnuN95T511SmvbtM+6su909rG+s6lGm1oeMORur7uc28xWoOi/eW6fd3WdfG9dTKrFMtLIZ5/7d7cM2if5aX579PKR1pmL+2PF9se25t3hcmuK23KB99bkopPdUw/44Jrq/qyw3THi7PZzQtYPu8ct7147bXlnPLn1Q+6jvV9k9Moj4zWbv3/aryfFcqn/aqlC+YPiLp5ZJeWya/sfz9aEppUxfbvkjSiZJ2pJQeaNjGo8qnSOYot7e6rSmlvQ3Tm9rKN5W/9K6z/R7bp3dRv25N9LMTubI8r+1YavJGbv8TFv3zU+X5sTbzn1D+8lyo/KGu+p8et9W4XErpOeVrJ73Y3bC+F8o/j7moanue7duVz51+TNJvS7pW0jXl8ZJS9OQe6zLTtXvfzy7PH69dNB1/SLqglDmtPJ9Vnh9Wd6L2Kkk7a2WrjmsnRautLGhNKF98v6fc+/mkpKdtP1IOLN5h213WuclEPzuRie7HXo3c/ufW2eGwv8fljjvqrGjq3XRjIsvdJOk65XO871W+oPpMSmlMkmzvUT5lMpkvgxkrpdTufW/1PL+q9l8KLc+2VteXSnVvQu0rpfQJ2/8k6Z3KY33eonxgca2k+21fmVI61EM9ev3stDtQnu792Ktp3/+ERf+0Brmd3Wb+q5WPzA8oX9icjD3l+aymmbZPljQdp37eVZ7fk1K6p1aHEyW9chrqMBO1AuKOlNLtXS7TGo9zTpflo/ZandeXAZzllOma8pDtNynfBXaZpOsl/W0/tlNxUNIJtk9KKe2rzTuzzTKPSzpPeT8+2Of6VI3c/uc0VP+0zhP/mu2m/XpteX6gxyOoqi3l+RLbr2iY/6uTXH+3frI8Nx39/oroUfRqfXl+V8dSx9qq3Mt4re2f67L8jySdbfvi+kzbP6189HlE+Tbqvksp/YfKF5ekCyuzxsrzZA9mWwdV5zbMu6LNMveW5+u63EavdR3m/d+IsOifO5WPAM6V9OHqecCS4O8rf/7lZDeUUtqpPJ5hofJ57fHzk7aXSvqjyW6jS9vL84211/t65fEa6M3nlS9KXmn7Y6WneIwyqOzXW3+nlA4q/6SEJH3G9oW18nNtX10pv19HjyT/xvZplbIvLfPmKV9kn9SvCNh+q+2r6oPFnAcWXl7+rG6jdSR93mS2qzyOQJI+VN227SuUT5s2WaM8ruUy23/u2uBH22favqgyaa9yYLxiIjdyDPn+b1tpHn16SLpYRwflbVe+K+l+xYPyLu2wzsYyyqegnijznlAelPcl5XO4dytf+EuSzqgtt0sTuKW2Mv+42xCVx4+MVV7vZ5XPsx8qr72nbc30R9O+bChzlqTvlLI/lPRvkj6jPHDre2X612vLWEdv0zws6YHyPnxF8aC853R08NneMu2/1X5Q2Lo29V5Z5q+sTLtJRweE3Vd5Hc+U6Q9LOqVSfoHyF3ZrbMCnlb/Er51IG1I+cGvdnvpIeW3fUD5av1ntB+X9bGUfPFU+T8cNyquUv7u1rvLa1qhyC7q6G5Q3NPu/7f4c9Adnpj2UzzN+SvnLekz5+sS9kn6+oexG9RgWZd4Zkm4vDfpAedM/VD5sL5aGvbC2TOOHLPrwqc0XnKRlyrfbPqPcrf6v0jjn9Lqtmf5oty8byi1SvotlS/mgjykfdX9N0kdURvk3LPeLyqeyvl9Z5n5Jv9VQdr7yUfZW5S/W/ZIeUu6dntRQvpcvq6WSPqx8pL+7tNVnlL+43y/p5Ib1/IzymIFnSzs+ZpvdtqHSPtcrj0v4kfJI76sVjI9QvjFjlfJB0P6y/HclfULS+bWyL1cOiN3K10mOWa/ahMUw7/+mR2tAD2aQcg50i6TvpJReN+j6ABh9XLMYUWWMwxsapp8raXX589PTWysAMxU9ixFVfpHyBeXu+HblbvJrlEeGzlM+F/q2lC98AsCkEBYjqtzZ8BHle6SXKP/A3I+Vz6t+VtKtqQyOA4DJIiwAACGuWQAAQiP5cx9u879fAf2SUhrI6HPaNqZar22bngUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABChAUAIERYAABC8wZdAQAYFpdffrkk6ZZbbpEkLVu2bHxeSmkgdRoW9CwAACGPYlraHr1KY6SklDyI7dK2B6vT9+HcuXMlSUeOHJmu6kyJXts2PQsAQIiwAACEuMANYFbr9lT84cOHJUn2QM5QDhw9CwBAiJ4FgFmp15t7qsvNnz9fknTw4MG+1GmY0bMAAIToWQCYNfo9VGBsbEyStHTpUknSjh07+rr+YULPAgAQIiwAACFGcKOjfraPUbrlkBHcM0vrttc5c6b2+Hj58uXj/968efOUbqtXjOAGAEwZLnBj3Cj2MoFOtmzZImnqexQtmzZtGv936zekWr8pNeroWQAAQoQFACDEBW5M2+knLnDHaNuTt2TJkvF/79y5c4A1Od4wfAa4wA0AmDJc4J6lRrFHCXRyww03SJJWr1494Jq01/rcDUMPY6LoWQAAQlyzmKUG8b6P0tEU1yxGQ7VNjcJ/d9r63E3Xrbxt6sA1CwDA1CAsAAAhLnADGFmDPJ3Ti1Grb9Xo1hwAMG3oWQAYWa1fk5WOXuwetpt2RunGjk7oWQAAQvQsZqmmo51hOyIDelFt24Ns0zOlR9FCzwIAECIsAAAhTkNhXKduM6eoMIpabfrAgQOSpAULFkzL9mYiehYAgBA9CwAz3sKFCyVJDz300Pi0Cy64oG/rH+XBdt2a+a8QADBphAUAIMRPlKMrk2kno3jRj58on/nmzp0rSTp06FDP65hNbZueBQAgxAVudGWio2JH8YgLs0vrd6WqF6e7+Q+UNmzYMGV1Gmb0LAAAIXoWmDB6DZhJqj3lTr9cu3btWknS9ddfPz0VGzL0LAAAIcICABDiNBQA1LROR51//vnj07Zt2zao6gwFehYAgBCD8oAGDMrDTMWgPADAlCEsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAECIsAAAhwgIAEHJKadB1AAAMOXoWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACBEWAIAQYQEACP0/XGAlK+tofDYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "original, y_original = get_specific_data(cuda=True)\n",
    "print(y_original)\n",
    "mu, sigma = vae.encoder.forward(x,vae.remap_y(y_original))\n",
    "B = 50\n",
    "zs = torch.cat([dist.Normal(mu.cpu(), sigma.cpu()).sample() for a in range(B)], 0)\n",
    "ys = torch.cat([vae.remap_y(y_original) for a in range(B)], 0)\n",
    "rs = vae.decoder.forward(zs.cuda(), ys).detach()\n",
    "compare_to_density(original,rs)\n",
    "\n",
    "y_new = torch.tensor(y_original)\n",
    "y_new[0,1] = (y[0,1] + 1) % 2\n",
    "print(y_new)\n",
    "zs = torch.cat([dist.Normal(mu.cpu(), sigma.cpu()).sample() for a in range(B)], 0)\n",
    "ys = torch.cat([vae.remap_y(y_new) for a in range(B)], 0)\n",
    "rs = vae.decoder.forward(zs.cuda(), ys).detach()\n",
    "compare_to_density(original,rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
