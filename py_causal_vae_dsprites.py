# -*- coding: utf-8 -*-
"""causal_vae_dsprites.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17TusqgELDORkovcWZxzlyScvCa7CgJyP

## Deep Causal Variational Inference

### Introduction:
To train a supervised variational autoencoder using Deepmind's [dSprites](https://github.com/deepmind/dsprites-dataset) dataset.

dSprites is a dataset of sprites, which are 2D shapes procedurally generated from 5 ground truth independent "factors." These factors are color, shape, scale, rotation, x and y positions of a sprite.

All possible combinations of these variables are present exactly once, generating N = 737280 total images.

Factors and their values:

* Shape: 3 values {square, ellipse, heart}
* Scale: 6 values linearly spaced in (0.5, 1)
* Orientation: 40 values in (0, 2$\pi$)
* Position X: 32 values in (0, 1)
* Position Y: 32 values in (0, 1)


Further, the objective of any generative model is essentially to capture underlying data generative factors, the disentangled representation would mean a single latent unit being sensitive to variations in single generative factors


### Goal:
To include the latent factors  as labels in the training and to invent a causal story that relates these factors and the images in a DAG.

Reference

[Structured Disentangled Representation](https://arxiv.org/pdf/1804.02086.pdf)
"""

#Install dependencies
!pip3 install pyro-ppl
!pip3 install torch torchvision
!pip3 install pydrive --upgrade
!pip3 install tqdm

# Load necessary libraries
from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns

import os

import torch
import torchvision.datasets as dset
import torch.nn as nn
import torchvision.transforms as transforms
from tqdm import tqdm
import pyro
from pyro.contrib.examples.util import print_and_log
import pyro.distributions as dist
from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate
from pyro.optim import Adam

# Change figure aesthetics
# %matplotlib inline
sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})

#to utilize GPU capabilities
USE_CUDA = True

pyro.enable_validation(True)
pyro.distributions.enable_validation(False)

# Mount Google drive to load data
from google.colab import drive
drive.mount('/content/gdrive')

# Mount G drive to access files
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Hack to get all available GPU ram.

import tensorflow as tf
tf.test.gpu_device_name()

!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

class Encoder(nn.Module):
  """
  Encoder module of VAE
  """
    def __init__(self, image_dim, label_dim, z_dim):
      """
      Constructor to initialize the layers of the encoder neural net

      args:
        image_dim: Input image dimension (X),
        label_dim: Label dimensions (Y) ,
        z_dim: Distribution of the latent representation


      """
      super(Encoder, self).__init__()
      #setup image and label dimensions from the dataset
      self.image_dim = image_dim
      self.label_dim = label_dim
      self.z_dim = z_dim
      # setup the three linear transformations used
      self.fc1 = nn.Linear(self.image_dim+self.label_dim, 1000)
      self.fc2 = nn.Linear(1000, 1000)
      self.fc31 = nn.Linear(1000, z_dim)  # mu values
      self.fc32 = nn.Linear(1000, z_dim)  # sigma values
      # setup the non-linearities
      self.softplus = nn.Softplus()

    def forward(self, xs, ys):
      """
      Forward prop of Encoder
      """
      # define the forward computation on the image xs and label ys
      # first shape the mini-batch to have pixels in the rightmost dimension
      xs = xs.reshape(-1, self.image_dim)
      #now concatenate the image and label
      inputs = torch.cat((xs,ys), -1)
      # then compute the hidden units
      hidden1 = self.softplus(self.fc1(inputs))
      hidden2 = self.softplus(self.fc2(hidden1))
      # then return a mean vector and a (positive) square root covariance
      # each of size batch_size x z_dim
      z_loc = self.fc31(hidden2)
      z_scale = torch.exp(self.fc32(hidden2))
      return z_loc, z_scale

class Decoder(nn.Module):

    def __init__(self, image_dim, label_dim, z_dim):
      """
      Constructor to initialize the layers of the decoder neural net
      """
      super(Decoder, self).__init__()
      # setup the two linear transformations used
      hidden_dim = 1000
      self.fc1 = nn.Linear(z_dim+label_dim, hidden_dim)
      self.fc2 = nn.Linear(hidden_dim, hidden_dim)
      self.fc3 = nn.Linear(hidden_dim, hidden_dim)
      self.fc4 = nn.Linear(hidden_dim, image_dim)
      # setup the non-linearities
      self.softplus = nn.Softplus()
      self.sigmoid = nn.Sigmoid()

    def forward(self, zs, ys):
      """
      Forward prop of Decoder
      """
      # define the forward computation on the latent z and label y
      # first concatenate z and y
      inputs = torch.cat((zs, ys),-1)
      # then compute the hidden units
      hidden1 = self.softplus(self.fc1(inputs))
      hidden2 = self.softplus(self.fc2(hidden1))
      hidden3 = self.softplus(self.fc3(hidden2))
      # return the parameter for the output Bernoulli
      # each is of size batch_size x 784
      loc_img = self.sigmoid(self.fc4(hidden3))
      return loc_img

class CVAE(nn.Module):

    def __init__(self, config_enum=None, use_cuda=False, aux_loss_multiplier=None):
      """

      """


      super(CVAE, self).__init__()

      self.image_dim = 64**2
      self.label_shape = np.array((1,3,6,40,32,32))
      self.label_names = np.array(('color', 'shape', 'scale', 'orientation', 'posX', 'posY'))
      self.label_dim = np.sum(self.label_shape)
      self.z_dim = 50
      self.allow_broadcast = config_enum == 'parallel'
      self.use_cuda = use_cuda
      self.aux_loss_multiplier = aux_loss_multiplier

      # define and instantiate the neural networks representing
      # the paramters of various distributions in the model
      self.setup_networks()

    def setup_networks(self):
        self.encoder = Encoder(self.image_dim, self.label_dim, self.z_dim)

        self.decoder = Decoder(self.image_dim, self.label_dim, self.z_dim)

        # using GPUs for faster training of the networks
        if self.use_cuda:
            self.cuda()

    def model(self, xs, ys):
        """
        The model corresponds to the following generative process:
        p(z) = normal(0,I)              # dsprites label (latent)
        p(y|x) = categorical(I/10.)     # which digit (supervised)
        p(x|y,z) = bernoulli(loc(y,z))   # an image
        loc is given by a neural network  `decoder`

        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """
        # register this pytorch module and all of its sub-modules with pyro
        pyro.module("cvae", self)

        batch_size = xs.size(0)
        options = dict(dtype=xs.dtype, device=xs.device)
        with pyro.plate("data"):

            prior_loc = torch.zeros(batch_size, self.z_dim, **options)
            prior_scale = torch.ones(batch_size, self.z_dim, **options)
            zs = pyro.sample("z", dist.Normal(prior_loc, prior_scale).to_event(1))

            # if the label y (which digit to write) is supervised, sample from the
            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)

            loc = self.decoder.forward(zs, self.remap_y(ys))
            pyro.sample("x", dist.Bernoulli(loc).to_event(1), obs=xs)
            # return the loc so we can visualize it later
            return loc

    def guide(self, xs, ys):
        """
        The guide corresponds to the following:
        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latent class from an image and the label
        loc, scale are given by a neural network `encoder`

        :param xs: a batch of scaled vectors of pixels from an image
        :return: None
        """
        # inform Pyro that the variables in the batch of xs are conditionally independent
        with pyro.plate("data"):
            # sample (and score) the latent handwriting-style with the variational
            # distribution q(z|x) = normal(loc(x),scale(x))

            loc, scale = self.encoder.forward(xs, self.remap_y(ys))
            pyro.sample("z", dist.Normal(loc, scale).to_event(1))

    def remap_y(self, ys):
        new_ys = []
        options = dict(dtype=ys.dtype, device=ys.device)
        for i, label_length in enumerate(self.label_shape):
            prior = torch.ones(ys.size(0), label_length, **options) / (1.0 * label_length)
            new_ys.append(pyro.sample("y_%s" % self.label_names[i], dist.OneHotCategorical(prior),
                                   obs=torch.nn.functional.one_hot(ys[:,i].to(torch.int64), int(label_length))))
        new_ys = torch.cat(new_ys, -1)
        return new_ys.to(torch.float32)

    def reconstruct_image(self, xs, ys):
        # backward
        sim_z_loc, sim_z_scale = self.encoder.forward(xs, self.remap_y(ys))
        zs = dist.Normal(sim_z_loc, sim_z_scale).to_event(1).sample()
        # forward
        loc = self.decoder.forward(zs, self.remap_y(ys))
        return dist.Bernoulli(loc).to_event(1).sample()

def setup_data_loaders(train_x, test_x, train_y, test_y, batch_size=128, use_cuda=False):
    train_dset = torch.utils.data.TensorDataset(
        torch.from_numpy(train_x.astype(np.float32)).reshape(-1, 4096),
        torch.from_numpy(train_y.astype(np.float32))
    )

    test_dset = torch.utils.data.TensorDataset(
        torch.from_numpy(test_x.astype(np.float32)).reshape(-1, 4096),
        torch.from_numpy(test_y.astype(np.float32))
    )
    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}

    train_loader = torch.utils.data.DataLoader(
        dataset=train_dset, batch_size=batch_size, shuffle=False, **kwargs
    )

    test_loader = torch.utils.data.DataLoader(
        dataset=test_dset, batch_size=batch_size, shuffle=False, **kwargs
    )

    return {"train":train_loader, "test":test_loader}

dataset_zip = np.load(
    '/content/gdrive/My Drive/data-science/causal-ml/projects/dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz',
    encoding = 'bytes',
    allow_pickle=True
)
imgs = dataset_zip['imgs']
labels = dataset_zip['latents_classes']
label_sizes = dataset_zip['metadata'][()][b'latents_sizes']
label_names = dataset_zip['metadata'][()][b'latents_names']

# Sample imgs randomly
indices_sampled = np.arange(imgs.shape[0])
np.random.shuffle(indices_sampled)
imgs_sampled = imgs[indices_sampled]
labels_sampled = labels[indices_sampled]

data_loaders = setup_data_loaders(
    imgs_sampled[1000:],
    imgs_sampled[:1000],
    labels_sampled[1000:],
    labels_sampled[:1000],
    batch_size=256,
    use_cuda=USE_CUDA
)

def train(svi, train_loader, use_cuda=False):
    # initialize loss accumulator
    epoch_loss = 0.
    # do a training epoch over each mini-batch x returned
    # by the data loader
    for xs,ys in train_loader:
        # if on GPU put mini-batch into CUDA memory
        if use_cuda:
            xs = xs.cuda()
            ys = ys.cuda()
        # do ELBO gradient and accumulate loss
        epoch_loss += svi.step(xs, ys)

    # return epoch loss
    normalizer_train = len(train_loader.dataset)
    total_epoch_loss_train = epoch_loss / normalizer_train
    return total_epoch_loss_train

def evaluate(svi, test_loader, use_cuda=False):
    # initialize loss accumulator
    test_loss = 0.
    # compute the loss over the entire test set
    for xs, ys in test_loader:
        # if on GPU put mini-batch into CUDA memory
        if use_cuda:
            xs = xs.cuda()
            ys = ys.cuda()
        # compute ELBO estimate and accumulate loss
        test_loss += svi.evaluate_loss(xs, ys)
    normalizer_test = len(test_loader.dataset)
    total_epoch_loss_test = test_loss / normalizer_test
    return total_epoch_loss_test

# Run options
LEARNING_RATE = 1.0e-3

# Run only for a single iteration for testing
NUM_EPOCHS = 10
TEST_FREQUENCY = 5

#################################
### FOR SAVING AND LOADING MODEL
################################
# clear param store

pyro.clear_param_store()

network_path = "/content/gdrive/My Drive/data-science/causal-ml/projects/trained_model.save"

PATH = "trained_model.save"

# new model
# vae = CVAE(use_cuda=USE_CUDA)

# save current model
# torch.save(vae.state_dict(), PATH)

# to load params from trained model
vae = CVAE(use_cuda=USE_CUDA)
vae.load_state_dict(torch.load(network_path))

"""### **DONT RUN THE BELOW CODE AS WE'VE ALREADY TRAINED THE MODEL AND WE'VE STORED THE NETWORK PARAMS**

## ==================================================================================
"""

import warnings
warnings.filterwarnings('ignore')

# clear param store
pyro.clear_param_store()

# setup the VAE
vae = CVAE(use_cuda=USE_CUDA)

# setup the optimizer
adam_args = {"lr": LEARNING_RATE}
optimizer = Adam(adam_args)

# setup the inference algorithm
svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())

train_elbo = []
test_elbo = []
# training loop

VERBOSE = True
pbar = tqdm(range(NUM_EPOCHS))
for epoch in pbar:
    total_epoch_loss_train = train(svi, data_loaders["train"], use_cuda=USE_CUDA)
    train_elbo.append(-total_epoch_loss_train)
    if VERBOSE:
        print("[epoch %03d]  average training loss: %.4f" % (epoch, total_epoch_loss_train))

    if epoch % TEST_FREQUENCY == 0:
        # report test diagnostics
        total_epoch_loss_test = evaluate(svi, data_loaders["test"], use_cuda=USE_CUDA)
        test_elbo.append(-total_epoch_loss_test)
        if VERBOSE:
            print("[epoch %03d] average test loss: %.4f" % (epoch, total_epoch_loss_test))

"""## ==================================================================================

#### Visualizing the reconstruction accuracy
"""

data_iter = iter(data_loaders["train"])
xs, ys = next(data_iter)
if USE_CUDA:
    xs = xs.cuda()
    ys = ys.cuda()
rs = vae.reconstruct_image(xs, ys)
if USE_CUDA:
    xs = xs.cpu()
    rs = rs.cpu()
originals = xs.numpy().reshape(-1, 64,64)
recons = rs.numpy().reshape(-1,64,64)

from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets

def f(x):
    fig = plt.figure()
    ax0 = fig.add_subplot(121)
    plt.imshow(originals[x], cmap='Greys_r',  interpolation='nearest')
    plt.axis('off')
    ax1 = fig.add_subplot(122)
    plt.imshow(recons[x], cmap='Greys_r',  interpolation='nearest')
    plt.axis('off')

interact(f, x=widgets.IntSlider(min=0, max=xs.shape[0], step=1, value=0));

y_names = ['shape', 'scale', 'orientation', 'posX', 'posY']
y_shapes = np.array((3,6,40,32,32))
img_dict = {}
for i, img in enumerate(imgs_sampled):
     img_dict[tuple(labels_sampled[i])] = img

def find_in_dataset(shape, scale, orient, posX, posY):
   fig = plt.figure()
   img = img_dict[(0, shape, scale, orient, posX, posY)]
   plt.imshow(img.reshape(64,64), cmap='Greys_r', interpolation='nearest')
   plt.axis('off')

interact(find_in_dataset,
         shape=widgets.IntSlider(min=0, max=2, step=1, value=0),
         scale=widgets.IntSlider(min=0, max=5, step=1, value=0),
         orient=widgets.IntSlider(min=0, max=39, step=1, value=0),
         posX=widgets.IntSlider(min=0, max=31, step=1, value=0),
         posY=widgets.IntSlider(min=0, max=31, step=1, value=0))


def SCM(vae, mu, sigma):
    z_dim = vae.z_dim
    Ny, Y, ys = [], [], []
    Nx = pyro.sample("Nx", dist.Uniform(torch.zeros(vae.image_dim), torch.ones(vae.image_dim)))
    Nz = pyro.sample("Nz", dist.Normal(torch.zeros(z_dim), torch.ones(z_dim)))
    m = torch.distributions.gumbel.Gumbel(torch.tensor(0.0), torch.tensor(1.0))
    for label_id in range(6):
        name = vae.label_names[label_id]
        length = vae.label_shape[label_id]
        new = pyro.sample("Ny_%s"%name, dist.Uniform(torch.zeros(length), torch.ones(length)) )
        Ny.append(new)
        gumbel_vars = torch.tensor([m.sample() for _ in range(length)])
        max_ind = torch.argmax(torch.log(new) + gumbel_vars).item()
        Y.append(pyro.sample("Y_%s"%name, dist.Normal(torch.tensor(max_ind * 1.0), 1e-4)))
#         Y.append(pyro.sample("Y_%s"%name, dist.Delta(torch.tensor(max_ind*1.0))))
        ys.append(torch.nn.functional.one_hot(torch.tensor(max_ind), int(length)))
    Y = torch.tensor(Y)
    ys = torch.cat(ys).to(torch.float32).reshape(1,-1).cuda()
    Z = pyro.sample("Z", dist.Normal(mu + Nz*sigma, 1e-4))
#     Z = pyro.sample("Z", dist.Delta(mu + Nz*sigma))
    zs = Z.cuda()
    p = vae.decoder.forward(zs,ys)
    X = pyro.sample("X", dist.Normal((Nx < p.cpu()).type(torch.float), 1e-4))
#     X = pyro.sample("X", dist.Delta((Nx < p.cpu()).type(torch.float)))
    return X, Y, Z

import warnings
warnings.filterwarnings("ignore")

xs, ys = next(data_iter)
x = xs[0].reshape(1,-1).cuda()
y = ys[0].reshape(1,-1).cuda()
mu, sigma = vae.encoder.forward(x,vae.remap_y(y))
mu = mu.cpu()
sigma = sigma.cpu()
recon_x1, y1, z1 = SCM(vae, mu, sigma)
print(y1)

from pyro.infer.importance import Importance
from pyro.infer.mcmc import MCMC
from pyro.infer.mcmc.nuts import HMC
from pyro.infer import SVI

"""
    Are we conditioning correctly?
    Should we be conditioning on just Y_shape or all Y attributes?
"""
intervened_model = pyro.do(SCM, data={"Y_shape": torch.tensor(0.)})

#Also should we just be conditioning on Y only or on all X, Y & Z?

conditioned_model = pyro.condition(SCM, data={
    "X": recon_x1,
#     "Z": z1,
    "Y_color":y1[0],
    "Y_shape": y1[1],
    "Y_scale": y1[2],
    "Y_orientation": y1[3],
    "Y_posX": y1[4],
    "Y_posY": y1[5]
   })

#  change to svi

"""
    HMC is not converging in our case and Importance is giving bad results.
    We are gonna try Variational Inference using SVI of pyro. If there is
    anything else we should try let us know.
"""

"""
    Also when we are conditioning on Y variables, they do are getting fixed
    (we checked by printing the Y tensor) but the image (i.e. X) is not
    following the same pattern (i.e. even though the Y_shape says ellipse,
    the image can be a heart). We are going to check if this is because of
    some issue in the VAE and that Z is not being trained correctly. Please
    give us your suggestions.
"""

posterior = pyro.infer.Importance(conditioned_model, num_samples = 100)
posterior.run(vae, mu, sigma)

result = []
for i in range(500):
  trace = posterior()
  x = trace.nodes['Nx']['value']
  ny_shape = trace.nodes['Ny_shape']['value']
  ny_scale = trace.nodes['Ny_scale']['value']
  ny_orientation = trace.nodes['Ny_orientation']['value']
  ny_posX = trace.nodes['Ny_posX']['value']
  ny_posY = trace.nodes['Ny_posY']['value']
  z = trace.nodes['Nz']['value']
  con_obj = pyro.condition(intervened_model, data = {
      "Nx": x,
      "Ny_shape": ny_shape,
      "Ny_scale": ny_scale,
      "Ny_orientation": ny_orientation,
      "Ny_posX": ny_posX,
      "Ny_posY": ny_posY,
      "Nz": z
  })

recon_x2,y2,z2 = con_obj(vae, mu, sigma)
print(y2)
recon_check(recon_x1.reshape(-1, 64, 64)[0], recon_x2.reshape(-1, 64, 64)[0])

# [ 0,  2,  1, 34,  4, 24]
def recon_check(original, recon):
  fig = plt.figure()
  ax0 = fig.add_subplot(121)
  plt.imshow(original, cmap='Greys_r',  interpolation='nearest')
  plt.axis('off')
  ax1 = fig.add_subplot(122)
  plt.imshow(recon , cmap='Greys_r', interpolation='nearest')
  plt.axis('off')


def show_density(img1, img2):
  fig = plt.figure()
  ax0 = fig.add_subplot(121)
  plt.imshow(img1.mean(axis=0), cmap='Greys_r',  interpolation='nearest')
  plt.axis('off')
  ax1 = fig.add_subplot(122)
  plt.imshow(img2.mean(axis=0) , cmap='Greys_r', interpolation='nearest')
  plt.axis('off')


def show_density2(imgs):
  _, ax = plt.subplots()
  ax.imshow(imgs.mean(), interpolation='nearest', cmap='Greys_r')
  ax.grid('off')
  ax.set_xticks([])
  ax.set_yticks([])


latents_bases = np.concatenate((label_sizes[::-1].cumprod()[::-1][1:],
                                np.array([1,])))

def latent_to_index(latents):
  return np.dot(latents, latents_bases).astype(int)

vae.label_names
